{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 3: Questions**\n",
    "\n",
    "This file is meant to contain all of your answers to the written questions of HW3. Please submit this alongside the rest of your code as a separate pdf labelled **questions.pdf**. \n",
    "\n",
    "> Note: Feel free to add code blocks and outputs within this file if you believe they help you in answering the questions but do **not** only screenshot your code and outputs from the homework notebook as a response!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Encoder-Decoder Model**\n",
    "\n",
    "For all the questions below, we would like to make it known that we expect your answers to be in the form of generics, not the specific numerical dimensions you have tuned your models to possess. Express your answers in terms of h: hidden dimension size, v: vocab size, e: embedding size, o: output dimension size, b: batch size and s: sentence length.\n",
    "\n",
    "For example, in question 3.4 when discussing the dimensions of last_hidden and last_cell_state, we state the dimensions are `(2, b, h)`, such that it is true for any general, correct, implementation of the model.\n",
    "\n",
    "It **is** valid, and sometimes expected, to express the generics in terms of a dimension multiplied by a constant e.g. write something such as `(2*e, 5*b)` (this is just an example no tensor in the model actually has this dimension), however all dimensions should only be expressed in terms of dimensional variables and constants. Any answer that only give specific numerical dimensions will not be given credit.\n",
    "\n",
    "### Q3.1:\n",
    "What are the limitations of converting semantic role labeling task to Question & Answer task (Model 2) using an Encoder-Decoder model? (max 4 sentences)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2\n",
    "In the initialization of your encoder model, you initialize a LSTM encoding layer and two linear projection layers. Give the dimensions of each of these layers and explain your reasoning. Please reference specific parts of your code in your answer. (max 6 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.3\n",
    "In the forward step of your encoder, you construct your tensor X by embedding the input. What should the resulting shape be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.4\n",
    "In the forward step of your encoder, why are last_hidden and last_cell_state of size (2, b = batch_size, h = hidden_size)? Why is the first dimension 2? Furthermore, why do we want to concatenate the forward and backwards tensors (and what is the shape of the concatenated output)? (3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.5\n",
    "In the forward step of your encoder, what is the dimension of the resulting tensor when we pass through the h_projection layer? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.6\n",
    "In the forward step of your encoder, what is the dimension of the resulting tensor when we pass through the c_projection layer? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.7\n",
    "In the initialization of your decoder layer, you initialize your actual decoder, and two projection layers. What are the dimensions with which you initialize all of these variables, and why do these dimensions make sense? Please have your answer include no more than two sentences per variable, and please include your code in the answer as you discuss the initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.8\n",
    "In the forward step of your decoder you should construct tensor Y by embedding the input. What should the resulting shape of Y be? (1 sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of Y should be h x 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.9\n",
    "In the forward step of your decoder, what is the dimension of the resulting tensor when passed through self.att_projection? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.10\n",
    "In the forward step of your decoder, over what dimension do we iterate, and what should the resulting shape of the new tensor be in relation to b and e where b is batch size and e is embedding size? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the second dimension which is time, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.11\n",
    "In the forward step of your decoder, what is the dimension of Ybar_t? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.12\n",
    "At the end of the forward step of your decoder, what is the shape of the single tensor made from stacking combined_outputs? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.13\n",
    "Discuss the manipulations to tensor dimensions that you performed in the attention step of the decoder step function (TODO 2), and why these were necessary. Please include direct references to the code you wrote, as well as the code you are referencing. (Maximum 4 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alpha_t = F.softmax(torch.matmul(enc_hiddens_proj, dec_hidden.unsqueeze(dim=2)), dim=1) #alpha_t = (b, L, 1)\n",
    "a_t = torch.matmul(torch.transpose(enc_hiddens, dim0=1, dim1=2), alpha_t) #a_t = (b, 2h, 1)\n",
    "a_t = a_t.squeeze(dim=2)\n",
    "U_t = torch.cat((dec_hidden, a_t), dim=1)\n",
    "```\n",
    "The code from the attention step of the decoder step function is above. We first need to add a dimension to `dec_hidden` because we want to find the dot product between it and `enc_hiddens_proj`, so we need matrix multiply them. We see that `enc_hiddens_proj` has dimension (b, L, h) and `dec_hidden` has dimension (b, h), so we add a dimension to make it (b, h, 1), then we get `alpha_t` with dimension (b, L, 1). Next, in order to matrix multiply `alpha_t` with `enc_hiddens` where the dimension is (b, L, 2h), we must transpose the second and third dimensions `enc_hiddens` to get a tensor of dimension (b, 2h, L), which now lets us multiply it with `alpha_t`. Now, `a_t` is dimension (b, 2h, 1) so we squeeze out the extra dimension to get `a_t` with dimension (b, 2h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.14\n",
    "In the decoder step function, what is the dimension of U_t? (One sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension of U_t is 3h x b."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4: Model Comparison & Analysis**\n",
    "\n",
    "### Q4.1:\n",
    "\n",
    "Compare two models above either using quantitative or qualitative analysis.\n",
    "\n",
    "The descriptive analysis can take one of two forms:\n",
    "\n",
    "1. _Nuanced quantitative analysis_ \\\n",
    "If you choose this option, you will need to further break down the quantitative statistics you reported initially. We provide some initial strategies to prime you for what you should think about in doing this. One possible starting point is to consider: if model $X$ achieves greater accuracy than model $Y$, to what extent is $X$ getting everything correct that $Y$ gets correct? For example, what's model's performance on each semantic role types?\n",
    "\n",
    "2. _Nuanced qualitative analysis_ \\\n",
    "If you choose this option, you will need to select individual examples and try to explain or reason about why one model may be getting them right whereas the other isnâ€™t. Are there any examples that both models get right or wrong and, if so, can you hypothesize a reason why this occurs?\n",
    "\n",
    "\n",
    "**NOTE:** The report should be written keeping both of the models in mind, discussing and comparing both of their performances, as well as doing the nuanced analysis with both of the models. Due to this, we won't be setting a hard limit on length of the report, but your report should be a substantial analysis.\n",
    "\n",
    "**CLARIFICATION:** Whichever option you take, we expect the following (at a minimum):\n",
    "\n",
    "1. A minimum of 3 clearly stated examples (for qualitative analysis) or statistics (for your quantitative analysis)\n",
    "2. An explanation as to why you think the phenomena you observed or talked about above is occurring.\n",
    "3. A discussion as to what conclusions you can draw about your models, particularly in comparison with each other, as a result of these examples or statistics.\n",
    "\n",
    "Please be clear with your responses, as we will grade according to the presence of the above.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... add your answer here*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
