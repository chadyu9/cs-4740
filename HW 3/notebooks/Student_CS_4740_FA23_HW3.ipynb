{"cells":[{"cell_type":"markdown","metadata":{"id":"YjT4AGnTJY9J"},"source":["# **Homework 3: Semantic Role Labeling using LSTM and Encoder-Decoder Architectures**\n","## CS4740/5740 Fall 2023"]},{"cell_type":"markdown","metadata":{"id":"7XJvjx33q-N7"},"source":["No part (code, documentation, comments, etc.) of this notebook or any assignment-related artefacts were generated/created, refined, or modified using generative AI tools such as Chat GPT. Cite this notebook as:\n","\n",">Sasha Boguraev ∗, Vinh Nguyen ∗, Vivian Nguyen ∗, Han Xia ∗, Travis Zhang ∗, Vivian Chen ∫, Gavin Fogel ∫,  Benjamin Hu ∫, Sienna Hu ∫. 2023. CS 4740 FA'23 HW3: Semantic Role Labeling using LSTM and Encoder-Decoder Architectures. GitHub. https://github.coecis.cornell.edu/cs4740-fa23-public/hw3-fa23/.\n","    ∗equal contribution, software creators, ordered alphabetically\n","    ∫equal contribution, software testers, ordered alphabetically\n","\n","Acknowledgments. This work is inspired from the assignment \"CS 4740 FA'22 HW3: Semantic Role Labeling\" developed by Heather Zheng and Han Xia, adapting from Ruoqi Zhang, Magd Bayoumi, Kai Zou, Wenyi Guo, Abigail See, Sahil Chopra, and Chris Manning"]},{"cell_type":"markdown","metadata":{"id":"FR5fsVH3q-N8"},"source":["\n","### Deadlines\n","\n","Please follow Ed #[] for updates on HW3 assignment, all the submission instructions and grouping information will be posted there; it can be misleading to just follow the \"git commit\" trail.\n","\n","Project submission deadline: **Wednesday November 8, 2023**.\n","\n","This assignment will **not** have a required milestone submission, but we strongly recommend completing the optional milestone by **Monday, October 30, 2023**.\n","\n","### Documentation\n","All documentation will be hosted within each python file. You do not need to read any documentation of classes you don't modify, but you should understand the general purpose of each method you use even if you don't modify said methods.\n","\n","Policies. All the policies described on the course website are applicable as is (including the policy on academic integrity and the use of generative AI tools), for more information, see: https://www.cs.cornell.edu/courses/cs4740/2023fa/.\n","\n","### Baselines\n","In the assignment, you will be implementing two models-- LSTMTagger and SRL Encoder-Decoder model. <br>\n","The baselines are: **LSTMTagger: 0.245** and **SRL Encoder-Decoder: 0.210**.\n"]},{"cell_type":"markdown","metadata":{"id":"-FDEcIxaKGrb"},"source":["# **Introduction**\n","---\n","\n","Semantic Role Labeling (**SRL**) is the task of automatically labelling the semantic roles of each argument according to each predicate in a passage.\n","\n","- **Predicates** (e.g., bought, sell, purchasing) represent events, and each sentence can have more than one predicate.\n","- **Semantic roles** express the abstract roles that predicate arguments can take in the event.\n","\n","Typical semantic arguments include Agent, Patient, and Instrument, and also adjunctive arguments indicating Locative, Temporal, Manner, Cause, and other aspects.\n","\n","Recognizing and labeling semantic arguments is a key task for answering \"Who\", \"When\", \"What\", \"Where\", and \"Why\" questions in Information Extraction, Question Answering, Summarization, and, in general, in all NLP tasks in which some kind of semantic interpretation is needed.\n","\n","The following sentence, exemplifies the annotation of semantic roles:\n","\n","[**Arg0 He**] wouldn't accept [**Arg1 anything of value**] from [**Arg2 those he was writing about**].\n","\n","Here, the roles for the **predicate 'accept'** (that is, the roleset of the predicate) are defined in the PropBank Frames scheme as:\n","\n","- **Arg0**: Agent (who/what is doing the event) <br></br>\n","- **Arg1**: Patient (who/what is the target of said event, usually direct object) <br></br>\n","- **Arg2**: Instrument, Benefactive, Attribute (the indirect object of said event, ex: how the event was done) <br></br>\n","- **ArgM-LOC**: Locative modifiers indicate where some action takes place. The notion of a locative is not restricted to physical locations, but abstract locations are being marked as LOC as well, as \"[in his speech]-LOC he was talking about …\".  <br></br>\n","- **ArgM-TMP**: Temporal ArgMs show when an action took place, such as \"in 1987\", \"last Wednesday\",\"soon\" or \"immediately\". Also included in this category are adverbs of frequency (eg. often always, sometimes), adverbs of duration (for a year/in an year), order (eg. first), and repetition (eg. again)..  <br></br>\n","\n","For this homework, the predicate is given for every sentence. Your task is to implement 2 models to predict A0/A1/A2/ArgM-LOC/ArgM-TMP.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BZvDpAz_tQ09"},"source":["## **Advice (please read)**\n","\n","---\n","1. We strongly encourage you to enable GPU support through Google Colab. To do so, go to `Runtime` --> `Change Runtime Type` --> Dropdown box below `Hardware Accelerator` --> `GPU`. This will help your models train *much* faster!\n","2. Please read through the entire notebook before you start coding. That might inform your code structure.\n","3. An assignment outline is found below; please consult it.\n","4. There are questions inside of the questions notebook that might be best answered as you concurrently work on the model. Before each section, we highly recommend looking ahead at the questions for that section and jotting down answers ahead of time!!!"]},{"cell_type":"markdown","metadata":{"id":"zZ1Mq2cCtJXG"},"source":["<a name=\"outline\"></a>\n","## **Assignment Outline and Grading Breakdown**\n","- [Part 0](#part0) Setting Everything Up\n","- [Part 1](#part1) Prepping the Data\n","- [Part 2](#part2) LSTM Encoder Model\n","  - [Implementation](#part2.1)\n","      - Initialization\n","      - Forward\n","  - [Entity Level F1 Score Calculation](#part2.2)  \n","  - [Questions](#part2.3)        \n","- [Optional Milestone](#milestone) Submission\n","- [Part 3](#part3) Encoder-Decoder Model\n","  - [Preprocess the Data](#part3.1)\n","  - [Implementation](#part3.2)\n","    - Encoder\n","      - Initialization\n","      - Forward\n","    - Decoder\n","      - Initialization\n","      - Forward\n","      - Step\n","  - [Entity Level F1 Score Calculation](#part3.3)\n","  - [Questions](#part3.4)\n","- [Part 4](#part4) Analysis\n","- [Part 5](#part5) Submission Creation\n","\n","\n","\n","**A brief note on the baselines:** To give you some breathing room for your implementation, our baselines are *not* perfect implementations. Correct implementations should therefore outperform our baselines."]},{"cell_type":"markdown","metadata":{"id":"NG7XK5EUQNfL"},"source":["<a name=\"part0\"></a>\n","[[^^^]](#outline)\n","# **Part 0: Setting Everything Up**\n","Assuming that you've followed setup.ipynb and successfully setup (or added a shortcut to) the hw3-fa23 folder, the following cells will install any external libraries and needed packages to run the HW3 assignment. Before proceeding, be sure to run the second code cell to ensure that the installation is successful.\n","\n","> Tip. It is possible to run out of GPU cycles on Colab, even if the GPU is sitting idle (but connected); we strongly recommend that you use CPU while you experiment, develop your code, then transition to using a GPU to run the final experiments."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183710,"status":"ok","timestamp":1698946422959,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"zCS-8YojYoJL","outputId":"dea54187-07f0-49c1-ff5e-947749b73b33"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/CS4740/hw3-fa23\n"]}],"source":["# Note that it would be normal for this cell to take noticably longer time given the size of the packages.\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","%cd \"/content/drive/MyDrive/CS4740/hw3-fa23\"\n","\n","from colab.file_utils import load_required\n","load_required(install_packages=[\"sentencepiece\", \"-U gensim\", \"torch==2.0.1\"])"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"elapsed":5142,"status":"ok","timestamp":1698946435846,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"YxX4clPFq-N9","outputId":"4d8acf1d-2b76-49b2-dc5c-5f696f748e3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[92mInstallation successful!\n","\u001b[0m\n"]},{"data":{"text/html":["<img src=\"https://media.giphy.com/media/QW5nKIoebG8y4/giphy.gif\"/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import display\n","\n","try:\n","    from utils.utils import success, colored\n","    print(colored(\"Installation successful!\\n\", \"green\"))\n","    display(success())\n","except ImportError:\n","    print(\"\\033[31mInstallation failed, please retrace your steps ...\")"]},{"cell_type":"markdown","metadata":{"id":"sxIBdOi6q-N9"},"source":["### Imports\n","A few imports that will be needed throughout this notebook are imported below. Within this notebook, feel free to import and/or install packages (a lot of the packages you may need should already be available) as you see fit;  \n",">**HOWEVER**, you are __not__ allowed to modify the imports in any of the Python source files; further, please do not modify (delete lines, change method signatures, etc.) above or below the `TODO` placeholders within the Python source files."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["### AUTORELOAD EXTENSION -- DO NOT MODIFY ###\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":591,"status":"ok","timestamp":1698947767051,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"xTLUWcq55-sG"},"outputs":[],"source":["from collections import Counter, namedtuple\n","from itertools import chain\n","import json\n","import math\n","import os\n","from pathlib import Path\n","import random\n","import time\n","from tqdm.notebook import tqdm, trange\n","from typing import List, Tuple, Dict, Set, Union\n","import sys\n","\n","# External imports\n","import gensim\n","import gensim.downloader as api\n","import nltk\n","import matplotlib.pyplot as plt\n","from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n","import numpy as np\n","import sentencepiece as spm\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","import torch.nn.utils\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","\n","# Internal imports\n","from srl.models.decoder import Decoder\n","from srl.models.encoder import Encoder\n","from srl.models.lstm_tagger import LSTMTagger\n","\n","from srl.train_and_eval.srl_trainer import srl_train_and_evaluate\n","from srl.train_and_eval.tagger_trainer import tagger_train_and_evaluate\n","\n","from srl.utils.constants import SRL_MAP, SRL_FRAMES\n","from srl.utils.srl_utils import *\n","from srl.utils.vocab import Vocab\n","\n","from srl.SRL import SRL\n","\n","from utils.styling import *\n","from utils.utils import *\n"]},{"cell_type":"markdown","metadata":{"id":"ce4DHFApq-N9"},"source":["#### Finally, we can grab the data we'll be using for this assignment:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1698947768856,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"IQ7XznlKq-N-"},"outputs":[],"source":["BASE_DIR = os.path.abspath(\"..\")\n","DATA_DIR = os.path.join(BASE_DIR, \"dataset\")\n","\n","with open(os.path.join(DATA_DIR,'train.json'), 'r') as f:\n","     train = json.loads(f.read())\n","\n","with open(os.path.join(DATA_DIR,'valid.json'), 'r') as f:\n","     val = json.loads(f.read())\n","\n","with open(os.path.join(DATA_DIR,'test.json'), 'r') as f:\n","     test = json.loads(f.read())"]},{"cell_type":"markdown","metadata":{"id":"ExAxhqz_eI5s"},"source":["<a name=\"part1\"></a>\n","# Part 1: Prepping the Data\n","Currently, the dataset has four fields:\n","1. 'text' represents the tokenized sentences from all documents (Type: `List[List[String]]` )\n","\n","2. 'verb_index' represents the **relative position** of the **predicate verb** in the sentence. (Type: `List[int]`)\n","  - Example:  [\"He\", \"would\", \"n't\", \"**accept**\", \"anything\", \"of\", \"value\", \"from\", \"those\", \"he\", \"was\", \"writing\", \"about\", \".\"]\n","    - verb_index = 3\n","\n","\n","3. 'srl_label' is the semantic role label of every token. (Type: `List[List[ String]]` )\n","\n","4. 'word_indices' is the index of every word token (same as howework 1&2 'index', Type: `List[List[int]]`)"]},{"cell_type":"markdown","metadata":{"id":"TgO9lcqnea7v"},"source":["We could try printing out the second training sentence as an example of the dataset.\n","The verb_index here is 4, which means the fifth word of the sentence, \"**coordinated**\", is the predicate."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1698947770301,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"HgA-rVa_eZ8U","outputId":"8c610792-099a-4a93-fd15-75d301e798a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n","['The', 'progress', 'of', 'this', 'coordinated', 'offensive', 'was', 'already', 'very', 'entrenched', 'by', 'then', '.']\n","[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n","['O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"]}],"source":["print(train['verb_index'][1])\n","print(train['text'][1])\n","print(train['words_indices'][1])\n","print(train['srl_frames'][1])"]},{"cell_type":"markdown","metadata":{"id":"L98pSVlDfXiW"},"source":["### **Converting Labels to Numerical Values**\n","In addition to converting our tokens into vector representations, we also need to convert our labels to numerical representations.\n","\n","For example, say we had two labels: \"O\", \"B-ARG0\". We could numerically represent these labels in a dictionary as Label \"0\" and Label \"1\": `{\"O\":0, \"B-ARG\":1}`.\n","\n","You can find this function, named **encode_srl_category**, inside of `srl/utils/srl_utils.py`\n"]},{"cell_type":"markdown","metadata":{"id":"Xd6R69UMH1rd"},"source":["We have also provided the `pad_sents` function, which pads all sentences to be the same length, specifically the length of the longest input sentence, using the provided `pad_token`.\n","\n","Finally, we have provided the `Vocab` class that represents the corpus as a `Vocab` object with several helper functions. These are used to preprocess the training set for you.\n","\n","You can find this `pad_sents` also in the `srl_utils.py` file and the `Vocab` class inside of `srl/utils/vocab.py`"]},{"cell_type":"markdown","metadata":{"id":"z_xBnjcygXEa"},"source":["# **Part 2: LSTM Encoder Model**\n","<a name=\"part2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"k-htQ0KOsqYw"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"7EcWsrx_RCVy"},"source":["<a name=\"part2\"></a>\n","\n","We will be using a **single layer LSTM** to predict Semantic Role Labels (**SRLs**) as our first model.\n","\n","Input: word tokens $\\vec{x}_1,\\vec{x}_2, \\dots, \\vec{x}_k$ and index $i$ of the verb ($\\vec{x}_i$) in the sentence\n","\n","Output: tags $\\vec{tag}_1,\\vec{tag}_2, \\dots, \\vec{tag}_k$ corresponding to the tag for a token $i$ in the sentence\n","\n","By the end, our LSTM model should be able to:\n","1.   Pass each word embedding to the LSTM and get the corresponding LSTM hidden layer. In the diagram above, each token's respective word hidden state is represented by the blue unlabeled rectangles with \"current token\" text. <br></br>\n","2.   Get hidden state of the predicate in the sentence (the green rectangle in the diagram above). In the diagram above, the hidden state of the predicate is represented by the green unlabeled rectangles with \"the predicate\" text. **Note!** They represent the same hidden state since we only have a singular predicate.  <br></br>\n","2.   Concatenate the hidden states with the predicate hidden state. <br></br>\n","3.   Pass the concatenated embedding through a linear layer to generate output <br></br>\n","\n","\n","\n","\n","\n","In this part, you need to implement an LSTM Encoder in PyTorch. Since you already understand how to implement RNNs from HW2, you are now allowed, **and encouraged** to use the torch.nn.LSTM function. https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n","\n","(**Tip**: read the documention and understand the parameter: \"batch_first\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1698947773823,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"iW4B2JkdC1pZ","outputId":"3bed7881-d083-47cc-ce72-f85882a50ba4"},"outputs":[{"name":"stdout","output_type":"stream","text":["initialize train vocabulary ..\n","number of word types: 19082, number of word types w/ frequency >= 1: 19082\n","number of unique words retained with remove_frac=0.3: 13358\n"]}],"source":["# Setting seed ***DO NOT MODIFY***\n","torch.manual_seed(123)\n","\n","print('initialize train vocabulary ..')\n","src_vocab = Vocab.from_corpus(train['text'], remove_frac=0.3)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1698947774919,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"4grDwdKKC3Ya"},"outputs":[],"source":["train_data = list(zip(train['text'],train['verb_index'],encode_srl_category(train['srl_frames'])))\n","val_data = list(zip(val['text'], val['verb_index'], encode_srl_category(val['srl_frames'])))"]},{"cell_type":"markdown","metadata":{"id":"S0NRPZgpWxg0"},"source":["<a name=\"l2\"></a>\n","\n","## 2.1 Implementing the LSTMTagger Class\n","Inside the `srl/models/lstm_tagger.py` file, you'll find an unfinished declaration of a LSTMTagger class.\n","\n","Work through each section of the class and be sure to handle all **### TODO**s (5 total)!\n","\n","- TODO 1: <br></br>\n","    - Initialize the layers of your model <br></br>\n","- TODO 2 - 5: <br></br>\n","    - Fill in the forward function of the LSTMTagger class that correctly faciliates each forward pass of the model"]},{"cell_type":"markdown","metadata":{"id":"JC5kjEbpW5-r"},"source":["#### **Train your LSTM Encoder Model**\n","\n","Once you've finished writing the LSTMTagger class, you can build the model and train it by using the cell below:\n","\n","> You can find the `tagger_train_and_evaluate` method inside of `srl/training_and_eval/tagger_trainer`"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"executionInfo":{"elapsed":129,"status":"error","timestamp":1698947777992,"user":{"displayName":"Joshua Huang","userId":"11981701065225751637"},"user_tz":240},"id":"x8k1_wQpgmdt","outputId":"fbfcf975-9161-485c-8a19-ee6535b71a6c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epochs:   0%|          | 0/10 [00:00<?, ?it/s]/Users/chadyu/Documents/cs-4740/HW 3/srl/models/lstm_tagger.py:86: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  out[i, t, :] = F.log_softmax(res)\n","10it [00:09,  1.11it/s]\n","Epochs:   0%|          | 0/10 [00:09<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/3t/g42v7z9n4v9_6986skczlc8r0000gp/T/ipykernel_46099/1269370445.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRL_MAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger_train_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/Documents/cs-4740/HW 3/srl/train_and_eval/tagger_trainer.py\u001b[0m in \u001b[0;36mtagger_train_and_evaluate\u001b[0;34m(number_of_epochs, model, train_data, val_data, criterion, min_loss, lr)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mloss_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mcur_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/cs-4740/HW 3/srl/train_and_eval/tagger_trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_data, optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/cs-4740/cs4740/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m~/Documents/cs-4740/cs4740/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["embed_dim = 64\n","hidden_dim = 64\n","weight = torch.ones(len(SRL_MAP))\n","weight[0] = 0.25\n","criterion = nn.NLLLoss(weight=weight)\n","lstm = LSTMTagger(src_vocab, embed_dim, hidden_dim=hidden_dim, output_dim=len(SRL_MAP), vocab_size=len(src_vocab)).to(get_device())\n","loss = tagger_train_and_evaluate(10, lstm, train_data, val_data, criterion, min_loss=0.2, lr= .05)"]},{"cell_type":"markdown","metadata":{"id":"vFGG0kU_SDuu"},"source":["<a name=\"part2.2\"></a>\n","## 2.2 Get Entity Level F1 Score on the Validation Set"]},{"cell_type":"markdown","metadata":{"id":"aKQXruq7Ix7f"},"source":["Run the cells below to calculate your F1 score on the validation set (no modifications needed):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JO1xrvnn0iFP"},"outputs":[],"source":["# Create predictions on validation data\n","inv_srl_map={SRL_MAP[key]:key for key in SRL_MAP}\n","val_predict = []\n","val_true = []\n","val_idx = []\n","\n","for idx in range(len(val_data)):\n","  out = lstm.forward([val_data[idx][0]], torch.tensor([val_data[idx][1]]))\n","  _, predicted = torch.max(out, 2)\n","\n","  len_sent = len(val_data[idx][0])\n","  result = predicted.cpu().numpy()[0]\n","\n","  for t in range(len_sent):\n","    val_predict.append(inv_srl_map[result[t]])\n","    val_true.append(inv_srl_map[val_data[idx][2][t]])\n","\n","  val_idx.extend(val['words_indices'][idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vc46fHOe58J6"},"outputs":[],"source":["# Get validation score\n","y_pred_dict = format_output_labels(val_predict, val_idx)\n","y_true_dict = format_output_labels(val_true, val_idx)\n","\n","print(str(mean_f1(y_pred_dict, y_true_dict)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0G4qIfig05W"},"outputs":[],"source":["# Save our model!\n","# Don't change saved model name here\n","\n","lstm.save_model(\"lstm.pth\")\n","lstm.load_model(\"lstm.pth\")"]},{"cell_type":"markdown","metadata":{"id":"ugThXwAoq-N_"},"source":["<a name=\"part2.3\"></a>\n","## 2.3 Questions\n","\n","Refer to the **Student_CS_4740_FA23_HW3_Questions** python notebook, Section 2."]},{"cell_type":"markdown","metadata":{"id":"VcOUMkZVq-N_"},"source":["<a name=\"milestone\"></a>\n","# [Optional] Milestone Submission\n","\n","Please run the code below to generate the submission zip file. Please make sure to specify both student's netid into `netid`. For instance, if my partner's and my netid are abc123 and cd234, `netid` should be abc123_cd234. While the milestone submission is optional, we strongly encourage you to submit your results to the Gradescope pseudo-scorer assignment to see your model's performance on the test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJMCykGVq-N_"},"outputs":[],"source":["# TODO: SET YOUR NETID AND PATH TO LSTM WEIGHTS\n","lstm_weights_path = \"\"\n","netid = \"abc123_cd234\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fAMgp8Msq-N_"},"outputs":[],"source":["!python make_submission.py --lstm_weights $lstm_weights_path --netid $netid"]},{"cell_type":"markdown","metadata":{"id":"cBBEWOqZRsoR"},"source":["<a name=\"part3\"></a>\n","# **Part 3: Encoder-Decoder Model**"]},{"cell_type":"markdown","metadata":{"id":"xgRpdeIeOG1z"},"source":["<a name=\"part3\"></a>\n","Now, one might ask how we can utilize an Encoder-Decoder model to do this same task. To faciliate this, we can convert the Semantics Role Labeling (SRL) task into the format of a Question and Answer task.\n","\n","#### For example, instead of trying to label a sentence like so:\n","\n","    [*Arg0 He*] would n't accept [*Arg1 anything of value*] from [*Arg2 those he was writing about*] .  \n","\n","#### We can instead feed a model a predicate and it's corresponding sentence:\n","\n","    \"accept\" + \"He wouldn't accept anything of value from those he was writing about.\";\n","\n","#### And ask the model: what is _ in this sentence? from our list of [ARG0, ARG1, ARG2, ARGM_TMP, ARGM_LOC]\n","\n","---\n","\n","We move to convert each possible tag into 5 question and answer pairs.\n","\n","Input: *predicate [SEPT] tokenized_seperated_sentence [SEPT] label we want to find*, where [SEPT] is a separator token.\n","\n","Output: the sequence that corresponds with that label, or empty if that label is not in this sentence.\n","\n","#### For Example:\n","\n","- Input 1: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARG0\n","  - Output 1: $<s>$ He $</s>$ <br></br>\n","\n","- Input 2: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARG1\n","  - Output 2: $<s>$ anything of value $</s>$ <br></br>\n","\n","- Input 3: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARG2\n","  - Output 3: $<s>$ $</s>$ (Explanation: because there's no ARG2 in this sentence) <br></br>\n","\n","- Input 4: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARGM-TMP\n","  - Output 4: $<s>$ $</s>$ <br></br>\n","\n","- Input 5: accept [SEPT] He would n't accept  anything of value from those he was writing about . ARGM-LOC\n","  - Output 5: $<s>$ $</s>$ <br></br>\n"]},{"cell_type":"markdown","metadata":{"id":"bRhhqVTYq-N_"},"source":["---\n","\n","## Beware!\n","The following section has a lot of mathematical notation but don't let that scare you! You might need to reread this section a couple of times in order to fully understand it, but that's okay! Don't try to force yourself to understand it all at once, rather, try to gain a better understanding behind the logic of this architecture as you build and debug your model.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"QLyEPxaMq-N_"},"source":["Given the input, we want to use a seq2seq model to predict output. In this section, we describe the training procedure for the proposed encoder-decoder system, which uses a `Bidirectional LSTM Encoder` and a `Unidirectional LSTM with Attention Decoder`. We'll recap the theoretical component here and in the modules where you are writing code, we will repeat the steps more explicitly in an algorithmic manner.\n","\n","<Insert diagram here>\n","\n","Given a sentence in the source language, we look up the word embeddings from an embeddings matrix, yielding $x_1,\\dots, x_n$ ($x_i \\in R^{e}$), where n is the length of the source sentence and e is the embedding size. We feed these embeddings to the bidirectional encoder, yielding hidden states and cell states for both the forwards (→) and backwards (←) LSTMs. The forwards and backwards versions are concatenated to give hidden states $h_i^{enc}$ and cell state $c_i^{enc}$:\n","\n","\n","$$h_i^{enc} = [\\overrightarrow{h_i^{enc}}; \\overleftarrow{h_i^{enc}}] \\text{ where }h_i^{enc} \\in R^{2h}, \\overrightarrow{h_i^{enc}}, \\overleftarrow{h_i^{enc}} \\in R^{h}$$\n","\n","$$c_i^{enc} = [\\overrightarrow{c_i^{enc}}; \\overleftarrow{c_i^{enc}}] \\text{ where }c_i^{enc} \\in R^{2h}, \\overrightarrow{c_i^{enc}}, \\overleftarrow{c_i^{enc}} \\in R^{h}$$\n","\n","\n","We then initialize the decoder’s first hidden state $h_0^{dec}$ with a linear projection of the encoder’s final hidden state\n","\n","$$h_0^{dec} = W_h[\\overrightarrow{h_n^{enc}}; \\overleftarrow{h_0^{enc}}] \\text{ where }h_0^{dec} \\in R^{h}, W_h \\in R^{h \\times 2h}$$\n","\n","And first cell state $c_0^{dec}$ with a linear projection of the encoder’s final cell state\n","\n","$$c_0^{dec} = W_c[\\overrightarrow{c_n^{enc}}; \\overleftarrow{c_0^{enc}}] \\text{ where }c_0^{dec} \\in R^{h}, W_c \\in R^{h \\times 2h}$$\n"]},{"cell_type":"markdown","metadata":{"id":"gx9RnIU2lkf-"},"source":["With the decoder initialized, we must now feed it a target sentence. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ word, $y_t \\in R^{e}$. We then concatenate $y_t$ with the combined-output vector $v_{t−1} \\in R^{h}$ from the previous timestep (we will explain in detail what this is later, but it is just the output from the previous step) to produce $\\bar{y_t} \\in R^{e+h}$. Note that for the first target (i.e. the start token), $v_0$ is usually a zero-vector (but it can be random or a learned vector as well). We then feed $\\bar{y_t}$ as input to the decoder.\n","\n","$$ h_t^{dec} = Decoder(\\bar{y_t}, (h_{t-1}^{dec},c_{t-1}^{dec}))\\text{ where }h_{t-1}^{dec} ∈ R^{h}$$\n","\n","We can take the decoder hidden state $h_t^{dec}$ and concatenate with an attention context vector $a_t$\n","$$u_t = [h_t^{dec},a_t] \\in R^{3h}$$\n","\n","Then, we pass $u_t$ through a linear layer and an activation function like tanh to obtain our combined-output vector $v_t$:\n","\n","$$v_t = W_v u_t \\text{ where } W_v \\in R^{h \\times 3h}, v_t \\in R^{h}$$\n","\n","Finally, we produce a probability distribution $P_t$ over target words at the $t^{th}$ timestep. Note, $V_{target}$ is the size of the target vocabulary.\n","\n","$$P_t = Softmax(W_{V_{target}} v_t) \\text{ where }P_t \\in R^{V_{target}}, W_{V_{target}}\\in R^{V_{target} \\times h}$$\n","\n","\n","During training, we compute the softmax cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the one-hot vector of the target word at timestep t:\n","\n","$$Loss(Model) = CrossEntropy(P_t, g_t)$$\n","\n","Now that we have described the model, let’s try implementing it for the SRL task!\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"rywV9ljcmaCd"},"source":["**How we get $a_t$ attention context vector:**\n","\n","First, at the beginning of the decoder step, we need to project encoder hidden states from $ R^{2h}$ to $ R^{h}$\n","\n","$$h_i^{enc-projection} = W_a h_i^{enc}, W_a \\in R^{h \\times 2h}, \\forall i$$\n","\n","Then at each decoder step, we compute dot product similarity between $h_t^{dec}$ and $h_i^{enc-projection}, \\forall i$:\n","$$score(h_t^{dec},h_i^{enc-projection}) = h_t^{dec} \\cdot h_i^{enc-projection}$$\n","Softmax the scores to create vector of weights:\n","$$\\alpha_t = softmax(score(h_t^{dec},h_i^{enc-projection}),\\forall i)$$\n","Take the weighted average over all encoder hidden states\n","$$a_t = \\alpha_t \\cdot h_t^{enc} \\in R^{2h}$$"]},{"cell_type":"markdown","metadata":{"id":"-k7efs_sOUbq"},"source":["<a name=\"part3.1\"></a>\n","## 3.1 Preprocess the data\n","\n","The code here converts raw dataset to the input and output format mentioned above. No modifications necessary."]},{"cell_type":"markdown","metadata":{"id":"7whI4ogUaCkl"},"source":["The following cell builds a combined vocab dictionary for input (source) and output (target). Both will share the same vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3U0qCNLdCDXk"},"outputs":[],"source":["train_src_corpus = generate_source_corpus(train['text'], train['verb_index'])\n","train_tgt_corpus = generate_target_corpus(train['text'], train['verb_index'], train['srl_frames'], train['words_indices'])\n","train_data = list(zip(train_src_corpus, train_tgt_corpus))\n","\n","val_src_corpus = generate_source_corpus(val['text'], val['verb_index'])\n","val_tgt_corpus = generate_target_corpus(val['text'], val['verb_index'], val['srl_frames'], val['words_indices'])\n","val_data = list(zip(val_src_corpus, val_tgt_corpus))\n","\n","#generate src_vocab\n","src_vocab = Vocab.from_corpus(np.array(train_src_corpus + val_src_corpus+train_tgt_corpus+val_tgt_corpus), remove_frac=0.3)\n","tgt_vocab = src_vocab"]},{"cell_type":"markdown","metadata":{"id":"9KOaMHNIakMJ"},"source":["You can print out our converted inputs & outputs here:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRysX2Vjajxx"},"outputs":[],"source":["print(f\"train['text'] is {train['text'][1]}\")\n","print(f\"train['srl_frames'] is {train['srl_frames'][1]}\")\n","print(f'train_src_corpus is {train_src_corpus[7]}')\n","print(f'train_tgt_corpus is {train_tgt_corpus[7]}')"]},{"cell_type":"markdown","metadata":{"id":"SKuf80-aNDpn"},"source":["The following cell loads pretrained GloVe embeddings and stores the embedding for each word in the vocabulary to `src_embeddings`. These embeddings will be used to initialize the learned embeddings in the models:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WudMPg-Gq79"},"outputs":[],"source":["model = api.load(\"glove-wiki-gigaword-300\")\n","torch.manual_seed(1)\n","\n","# Obtain src_embeddings\n","src_embeddings = []\n","for i, word in enumerate(src_vocab.word2id.keys()):\n","    try:\n","        src_embeddings.append(model[word])\n","    except:\n","        src_embeddings.append(torch.rand(300))\n","\n","src_embeddings = np.stack(src_embeddings, 0)\n","src_embeddings = torch.from_numpy(src_embeddings)"]},{"cell_type":"markdown","metadata":{"id":"XmtF1G14JUjo"},"source":["<a name=\"part3.2\"></a>\n","## 3.2 Implementation\n","Your next task is to implement the Encoder-Decoder model by completing the **#TODOs** found in `srl/models/encoder.py` (5 TODOS) and `srl/models/decoder.py` (5 TODOS).\n","\n","For the `Encoder`:\n","- TODO 1: <br></br>\n","    - Initialize the layers of your model <br></br>\n","- TODO 2 - 5: <br></br>\n","    - Fill in the forward function of the `Encoder` class that correctly faciliates each forward pass of the model\n","\n","For the `Decoder`:\n","- TODO 1: <br></br>\n","    - Initialize the layers of your model <br></br>\n","- TODO 2: <br></br>\n","    - Using what you've learned from writing the forward functions for both the `LSTMTagger` and the `Encoder`, fill in the forward function of the `Decoder` class <br></br>\n","- TODO 3-5: <br></br>\n","    - Write the code that facilitates a single step done by the model after each time step.\n"]},{"cell_type":"markdown","metadata":{"id":"ZNF0GlCiNtZu"},"source":["Once you've finished implementing both classes. We can now use the SRL class from `srl/SRL.py` which puts together the LSTM Encoder and Decoder with several helper methods that allow the model to generate outputs.\n","\n","**No** modifications to the SRL class are necessary."]},{"cell_type":"markdown","metadata":{"id":"Bu72LSMXOetE"},"source":["Run the following cells to train and save your model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DDPC2jpPTij"},"outputs":[],"source":["embed_size = 300\n","hidden_size = 512\n","\n","np.random.seed(1234)\n","torch.manual_seed(1234)\n","\n","epochs = 8\n","train_batch_size = 128\n","clip_grad = 2\n","log_every = 100\n","valid_niter = 500\n","model_save_path=\"srl.ckpt\"\n","device = get_device()\n","\n","model = SRL(\n","    embed_size,\n","    hidden_size,\n","    src_vocab,\n","    tgt_vocab,\n","    device=device,\n","    pretrained_source=src_embeddings\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3DXG6QhPVGb"},"outputs":[],"source":["model.to(device)\n","model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hY9P5WwFPWHs"},"outputs":[],"source":["# Define each of the variables then you can run this command!\n","srl_train_and_evaluate(\n","    model,\n","    train_data,\n","    val_data,\n","    optimizer,\n","    train_batch_size,\n","    clip_grad,\n","    log_every,\n","    valid_niter,\n","    model_save_path,\n","    epochs\n",")"]},{"cell_type":"markdown","metadata":{"id":"ySuu1gWD-eqH"},"source":["<a name=\"part3.3\"></a>\n","## 3.3 Get Entity level F1 score on the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwBmPKzffv59"},"outputs":[],"source":["# First load the model in\n","model = SRL.load(model_save_path)\n","model.to(get_device())"]},{"cell_type":"markdown","metadata":{"id":"_9oQzHeDq-OB"},"source":["We can now use an approximate search algorithm called **beam search** to generate our model outputs and transform them to the same format as our LSTMTagger model so that we can compare their performance.\n","\n","#### Run the cells below to generate the new model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OC4N1qlfxFZ"},"outputs":[],"source":["# Use beam search to generate output\n","val_data_src = [src for src, _ in val_data]\n","val_pred = generate_predictions_using_beam_search(model, val_data_src)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8PYYaC8n-0KU"},"outputs":[],"source":["y_pred_model2_dict = format_output_labels(val_pred, val_idx)\n","y_true_model2_dict = format_output_labels(val_true, val_idx)\n","print(str(mean_f1(y_pred_model2_dict, y_true_model2_dict)))"]},{"cell_type":"markdown","metadata":{"id":"Tx1HAL8Ih49w"},"source":["<a name=\"part3.4\"></a>\n","## 3.4 Questions\n","\n","Refer to the **Student_CS_4740_FA23_HW3_Questions** python notebook, Section 3."]},{"cell_type":"markdown","metadata":{"id":"9ZKLsoeKR-8s"},"source":["<a name=\"part4\"></a>\n","# **Part 4: Analysis**\n","\n","Refer to the **Student_CS_4740_FA23_HW3_Questions** python notebook, Section 4."]},{"cell_type":"markdown","metadata":{"id":"CgeQbxQXM2uH"},"source":["<a name=\"part5\"></a>\n","# **Part 5: Submission Creation**\n","\n","Run the following code below to create a submission zip file. Please make sure to specify both student's netid into `netid`. For instance, if my partner's and my netid are abc123 and cd234, `netid` should be abc123_cd234.\n","\n","**Note**: The predictions for the Encoder-Decoder model does take a noticeable amount of time, so don't worry if the cell below runs for a while"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqNhc0D6q-OB"},"outputs":[],"source":["# TODO: SET YOUR NETID AND PATH TO LSTM WEIGHTS AND SRL WEIGHTS\n","lstm_weights_path = \"\"\n","srl_weights_path = \"\"\n","netid = \"abc123_cd234\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6b2rJ3dRq-OB"},"outputs":[],"source":["!python make_submission.py --lstm_weights $lstm_weights_path --srl_weights $srl_weights_path --netid $netid"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
