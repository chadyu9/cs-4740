{"cells":[{"cell_type":"markdown","metadata":{"id":"f361852d"},"source":["## CS 4740 Fa'23 HW2: Named-entity recognition using FFNNs and RNNs\n","\n","CS 4740/CS 5740/COGST 4740/LING 4474, fall 2023\n","\n","<br/>\n","\n","<div align=\"center\">\n","    <img src=\"https://nebusresearch.files.wordpress.com/2019/05/barney-and-clyde_gene-wingarten-dan-weingarten-david-clark_22-may-2019.gif\" width=\"700\"/>\n","    <br/>\n","    Source: <a href=\"https://www.gocomics.com/barneyandclyde/2019/05/22\">\n","\"<i>Barney & Clyde</i>\" by Gene Weingarten, Dan Weingarten, and David Clark for May 22, 2019</a>\n","    <br/>\n","    (Distributed under CC BY-NC-SA 3.0 US.)\n","</div>\n","\n","<br/>\n","\n","No part (code, documentation, comments, etc.) of this notebook or any assignment-related artefacts were generated/created, refined, or modified using generative AI tools such as Chat GPT. Cite this notebook as:\n","> Tushaar Gangavarapu, Pun Chaixanien<sup>&lowast;</sup>, Kai Horstmann<sup>&lowast;</sup>, Dave Jung<sup>&lowast;</sup>, Aaishi Uppuluri<sup>&lowast;</sup>, Lillian Lee, Darren Key<sup> &int;</sup>, Logan Kraver<sup> &int;</sup>, Lionel Tan<sup> &int;</sup>. 2023. CS 4740 Fa'23 HW2: Named-entity recognition using FFNNs and RNNs. GitHub. https://github.coecis.cornell.edu/cs4740-fa23-public/hw2-fa23/. <br/>\n","> &nbsp;&nbsp;&nbsp;&nbsp;<sup>&lowast;</sup>equal contribution, software creators, ordered alphabetically<br/>\n","> &nbsp;&nbsp;&nbsp;&nbsp;<sup>&int;</sup>equal contribution, software testers, ordered alphabetically<br/>\n","\n","__Acknowledgments.__ This work is inspired from the assignment \"CS 4740 FA'22 HW2: Neural NER\" developed by John Chung, Renee Shen, John R. Starr, Tushaar Gangavarapu, Fangcong Yin, Shaden Shaar, Marten van Schijndel, and Lillian Lee."],"id":"f361852d"},{"cell_type":"markdown","source":["---\n","\n","__Deadlines__\n","\n","Please follow [Ed #263](https://edstem.org/us/courses/42759/discussion/3482317) for updates on HW2 assignment, all the submission instructions and grouping information will be posted there; it can be misleading to just follow the \"git commit\" trail.\n","* Milestone deadline: <font color=\"red\">October 02, 2023</font>, 11.59pm on the submission site(s).\n","* Project submission deadline: <font color=\"red\">October 19, 2023</font>, 11.59pm on the submission site(s).\n","\n","Here's our rationale for having a milestone, and the chosen date for the milestone:\n","\n","* accounting for Yom Kippur, we have set the milestone deadline to be 10/03 (which would've otherwise been 09/29); we want you to be able to enjoy the holiday without having to worry about the assignment!,\n","* we understand that it's the prelim season; however, we really want you to get started on the assignment, so you have time to experiment with the models (and associated hyperparameters) after the prelim.\n","\n","The milestone submission _is_ graded on correctness<a name=\"footnote1\"></a>[<sup>[1]</sup>](#milestone-note) and not just completion—why?: we want you to develop a way of \"PyTorch thinking\" right from the start of the assignment. You are expected to complete up to (and including) [training and evaluation (section 3)](#sec3) for the milestone submission.\n","\n","> <a name=\"milestone-note\"></a><sup>[1] </sup>Passing the milestone doesn't guarantee full correctness of the tested components (you should be writing your own test cases to assure that!). Upon final submission, your code will be tested on several additional test cases. [↩︎](#footnote1)"],"metadata":{"id":"aEPY9seimQMF"},"id":"aEPY9seimQMF"},{"cell_type":"markdown","source":["__Documentation.__ For your convenience, we're maintaining a documentation of all the modules and scripts used in HW2 assignment at: https://pages.github.coecis.cornell.edu/cs4740/hw2-fa23/."],"metadata":{"id":"sa0bWjgvmPJG"},"id":"sa0bWjgvmPJG"},{"cell_type":"markdown","source":["__Learning outcomes__\n","\n","The goal of this assignment is to model the problem of named-entity recognition as a classification task and use two neural approaches to solve it. To this end, you will:\n","* understand and process data as needed (tokenization, padding, truncation, etc.),\n","* generate learnable latent representations for tokens (embeddings),\n","* implement a feed-forward network for classification,\n","* implement a recurrent network for classification, and\n","* analyze activations and contrast model performances.\n","\n","To enable (performance) comparison across approaches, we will be using the same dataset and data splits as in HW1."],"metadata":{"id":"m2RkZnzeRP-G"},"id":"m2RkZnzeRP-G"},{"cell_type":"markdown","source":["__Policies.__ All the policies described on the course website are applicable as is (including the policy on academic integrity and the use of generative AI tools), for more information, see: https://www.cs.cornell.edu/courses/cs4740/2023fa/."],"metadata":{"id":"dFeQppedRTMR"},"id":"dFeQppedRTMR"},{"cell_type":"markdown","source":["---\n","\n","<a name=\"outline\"></a>__Assignment outline__\n","\n","* [[$\\ast$] Attributions](#attr)\n","* [[0] Imports and installs!](#sec0)\n","* [[1] Data processing](#sec1)\n","  * [[1.1] Tokenization](#sec11)\n","  * [[1.2] Data collation](#sec12)\n","* [[2] Embeddings](#sec2)\n","* [[3] The training, evaluation loop](#sec3)\n","* [[$\\ast$] Milestone submission](#milestone)\n","* [[4] FFNNs](#sec4)\n","  * [[4.1] Single-layer FFNN](#sec41)\n","  * [[4.2] Multi-layered FFNN](#sec42)\n","  * [[$\\ast$] Leaderboard submission](#leaderboard_ffnn) ← optional!<a name=\"footnote2\"></a>[<sup>[2]</sup>](#optional)\n","  * [[4.3] Analyzing FFNN](#sec43)\n","* [[5] RNNs, or \"multilayer machines with loops!\"](#sec5)\n","  * [[5.1] Single-layer vanilla RNN](#sec51)\n","  * [[5.2] Multi-layered RNN](#sec52)\n","  * [[$\\ast$] Leaderboard submission](#leaderboard_rnn) ← optional!<a name=\"footnote2\"></a>[<sup>[2]</sup>](#optional)\n","  * [[5.3] Analyzing RNN](#sec53)\n","* [[$\\ast$] Final submission](#final)\n","\n","> <a name=\"optional\"></a><sup>[2] </sup>The leaderboard submission is private (your scores are not visible to other students) and using the leaderboard is optional.\n",">\n","> That said, we _will_ run your models, both FFNN and RNN ([from your final submission](#final)), and you will be graded on the submitted models' test performance, measured as weighted-average entity-level F1. <font color=\"red\">To receive full credit, your FFNN must beat the baseline of $0.40$ and RNN must beat the baseline of $0.65$.</font> If the model scores below the set baseline, the associated performance points will be a linear function of the score [= being close to the baseline guarantees majority of the credit]. [↩︎](#footnote2)"],"metadata":{"id":"8gk-t-bGRVZZ"},"id":"8gk-t-bGRVZZ"},{"cell_type":"markdown","source":["---\n","\n","<a name=\"attr\"></a>\n","\n","### [$\\ast$] Attributions <small>[↩︎](#outline)</small>\n","\n","Please use the space provided below to acknowledge (by name/source) all help you received (this includes generative AI tools). You're welcome to cite references in line when answering the \"written\" questions."],"metadata":{"id":"ldbIJLyN7CTC"},"id":"ldbIJLyN7CTC"},{"cell_type":"markdown","source":["_Attributions (if any) go here._"],"metadata":{"id":"MvCMmrPd7Lnj"},"id":"MvCMmrPd7Lnj"},{"cell_type":"markdown","source":["---\n","\n","<a name=\"sec0\"></a>\n","### [0] Imports and installs! <small>[↩︎](#outline)</small>\n","\n","Assuming that you've followed [setup.ipynb](https://github.coecis.cornell.edu/cs4740-fa23-public/hw2-fa23/blob/main/notebooks/setup.ipynb) and successfully setup (or added a shortcut to) the `hw2-fa23` folder, the following code will install any external libraries and needed packages to run HW2 assignment. Before proceeding, be sure to run the second code cell to ensure that the installation is successful.\n","\n","> __Tip.__ It is possible to run out of GPU cycles on Colab, even if the GPU is sitting idle (but connected); we strongly recommend that you use CPU while you experiment, develop your code, then transition to using a GPU to run the final experiments."],"metadata":{"id":"dGIpcJ5rnhfn"},"id":"dGIpcJ5rnhfn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d625230f"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","%cd \"/content/drive/MyDrive/CS4740/hw2-fa23\"\n","\n","from colab.file_utils import load_required\n","load_required(install_packages=[\"datasets\", \"torchinfo\", \"jsonlines\"])"],"id":"d625230f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fSPMiqQl4ph"},"outputs":[],"source":["from IPython.display import display\n","\n","try:\n","    from ner.utils.utils import success, colored\n","    print(colored(\"Installation successful!\\n\", \"green\"))\n","    display(success())\n","except ImportError:\n","    print(\"\\033[31mInstallation failed, please retrace your steps ...\")"],"id":"2fSPMiqQl4ph"},{"cell_type":"markdown","metadata":{"id":"c10c8c64"},"source":["A few imports that will be needed throughout this notebook are imported below. Within this notebook, feel free to import and/or install packages (a lot of the packages you may need should already be available) as you see fit; <font color=\"red\">however, you are __not__ allowed to modify the imports in any of the Python source files; further, please do not modify (delete lines, change method signatures, etc.) above or below the `TODO` placeholders within the Python source files.</font>"],"id":"c10c8c64"},{"cell_type":"code","execution_count":null,"metadata":{"id":"851e1fc7"},"outputs":[],"source":["import os\n","from collections import Counter\n","from itertools import chain\n","\n","import datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn.functional as F\n","import yaml\n","from IPython.display import display\n","\n","from ner.data_processing.constants import NER_ENCODING_MAP\n","from ner.data_processing.tokenizer import Tokenizer\n","from ner.models.ner_predictor import NERPredictor\n","from ner.utils.utils import success, colored\n","from ner.utils.visualize import inspect_preds, visualize_activations\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format=\"retina\""],"id":"851e1fc7"},{"cell_type":"markdown","source":["Let's setup a few paths! For convenience, we will redirect all the output artefacts to the `CS4740/hw2-fa23/artefacts` directory—this includes processed dataset, tokenizer files, experimental artefacts (configs, saved checkpoints, trained models, etc.), submission zip files, etc."],"metadata":{"id":"yhsYcsGDM4Io"},"id":"yhsYcsGDM4Io"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b22adf8"},"outputs":[],"source":["BASE_DIR = os.path.abspath(\".\")\n","ARTEFACTS_DIR = os.path.join(BASE_DIR, \"artefacts\")\n","SCRIPTS_DIR = os.path.join(BASE_DIR, \"scripts\")\n","CONFIGS_DIR = os.path.join(BASE_DIR, \"scripts/configs\")"],"id":"0b22adf8"},{"cell_type":"markdown","source":["Finally, set your and your partner's net IDs as a comma-separated string (e.g., \"`<net-id-1>`, `<net-id-2>`\"); we'll use the `net_ids` variable to auto-populate any required information while making the submission."],"metadata":{"id":"jCbJDDsdePNh"},"id":"jCbJDDsdePNh"},{"cell_type":"code","source":["# Add your net IDs as a comma-separated string (e.g., \"<net-id-1>, <net-id-2>\").\n","net_ids = None\n","\n","if net_ids is None:\n","    raise ValueError(\"net-IDs not set; set them above\")"],"metadata":{"id":"9MQNEzFZeN28"},"id":"9MQNEzFZeN28","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"737c100d"},"source":["---\n","\n","<a name=\"sec1\"></a>\n","### [1] Data processing <small>[↩︎](#outline)</small>\n","\n","As noted earlier, we will be using the same dataset as in HW1—the dataset .json files are located in the `hw2-fa23/dataset` folder. For convenience, we will be converting these .json files into an [arrow dataset](https://huggingface.co/docs/datasets/about_arrow). For the purposes of this assignment, we'll walk you through data access with an arrow dataset (don't worry, it's as simple as accessing a dictionary!).\n","\n","Run the cell below to convert the .json data files into an arrow dataset (for those curious about the script, please see the documentation website). The processed dataset will be stored in `CS4740/hw2-fa23/artefacts/dataset` folder.\n","\n","> __Tip.__ For any of the scripts provided, you can run `!<command-name> --help` to see the arguments of the command! <br/>\n","> (Replace the `<command-name>` accordingly.)"],"id":"737c100d"},{"cell_type":"code","source":["!create_hf_dataset.py \\\n","    --basepath-to-dataset-json-files={os.path.join(BASE_DIR, \"dataset\")} \\\n","    --path-to-store-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")}"],"metadata":{"id":"gqQhRRxwTXP9"},"id":"gqQhRRxwTXP9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> __Tip.__ (This tip applies to all commands that save run artefacts.) The above cell stores the run artefact (here, dataset) and you don't need to rerun the above cell to re-populate the artefact; instead, just load the artefact from the `CS4740/hw2-fa23/artefacts` folder) as shown below."],"metadata":{"id":"6jUhbOO4TZUc"},"id":"6jUhbOO4TZUc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2af2021b"},"outputs":[],"source":["hf_dataset = datasets.load_from_disk(os.path.join(ARTEFACTS_DIR, \"dataset\"))\n","print(hf_dataset)"],"id":"2af2021b"},{"cell_type":"markdown","source":["The following cell shows how to access a specific split [= \"train\", \"val\", or \"test\"] of the `hf_dataset`, and a specific sample (accessed by index). You should observe that each sample of train/val splits includes three fields: \"text\", \"index\", and \"NER\", while the test split has two fields: \"text\" and \"index\". (This is consistent with what you observed in HW1.)\n","\n","Go on, try to access the \"text\" and \"NER\" fields of the chosen sample; what's the datatype of the \"text\" field?"],"metadata":{"id":"pKS8_3v3ofjq"},"id":"pKS8_3v3ofjq"},{"cell_type":"code","source":["split, sample_idx = \"train\", 5\n","hf_dataset[split][sample_idx]\n","\n","# Try to access \"text\" and \"NER\" fields of the chosen sample."],"metadata":{"id":"WrQezAyRory_"},"id":"WrQezAyRory_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One final functionality to note is: [`map`](https://huggingface.co/docs/datasets/about_map_batch)—you're not expected to know how to use this, but that it exists and is used to apply a function to the instances of an arrow dataset. Why should you care?: `map` lets us parallelly process a batch of data instances!\n","\n","For now, that's all about (cool) arrow datasets, we'll revisit them in HW4 in much more detail!"],"metadata":{"id":"VR3KLDOdoqlG"},"id":"VR3KLDOdoqlG"},{"cell_type":"markdown","source":["<a name=\"sec11\"></a>\n","#### [1.1] Tokenization <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/data_processing/tokenizer.py`.\n","\n","From the previous section, you would notice that the \"text\" field is of type `List`, i.e., the text is already given to you as tokens! But wait, these are strings, and we need numerical data to use neural approaches (why?). Before proceeding, it is worth looking at the class methods (provided to you) in the `Tokenizer` class: you will find the documentation website to be extemely helpful.\n","\n","You will be completing <font color=\"orange\">`TODO-1.1-1`</font> in the `tokenize()` method; pay attention to the class constructor and class variables. What is expected from `tokenize()`?:\n","* note from the input type of `input_seq` that it can be a `str` or `List[str]`—to keep consistent, if it's a string, convert it to a list of space-separated strings,\n","* `self.lowercase` is a class variable that determines tokenizer's case-handling behavior; be sure to handle this while tokenizing, and\n","* we need to \"somehow\" represent strings as integers (a.k.a, input IDs); the `Tokenizer` maintains `self.token2id` for this specific purpose; use [`torch.LongTensor`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftensors.html) to convert a list of input IDs into a tensor of _integer_ values.\n","\n","For now, let's return a dictionary with `input_ids` as the key and associated `LongTensor` of input IDs as the value. Run the cell below to test that everything runs as expected; we strongly recommend that you modify the code in the cell below to add your own tests."],"metadata":{"id":"7mEQVUyjq6JQ"},"id":"7mEQVUyjq6JQ"},{"cell_type":"code","source":["from ner.data_processing.constants import PAD_TOKEN, UNK_TOKEN\n","from ner.data_processing.tokenizer import Tokenizer\n","\n","tokenizer = Tokenizer(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN, lowercase=False)\n","tokenizer.from_dict({PAD_TOKEN: 0, UNK_TOKEN: 1, \"I\": 2, \"am\": 3, \"Naruto\": 4})  # stub the tokenizer\n","tokenizer.tokenize(input_seq=\"I am Naruto Uzumaki\")"],"metadata":{"id":"0oZZRz8HwOcs"},"id":"0oZZRz8HwOcs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that the basic tokenization runs as expected, let's move on to handling the `max_length` constraint. The length of the sequence to be returned must exactly match the provided `max_length`, i.e., if `max_length` is provided, (if needed,) we need to either pad the sequence with `self.pad_token`, or truncate the sequence. Pay special attention to the `padding_side` and `truncation_side` parameters; these indicate which side to pad/truncate from.\n","\n","To keep track of what is padded and what isn't, we will also return a padding mask of `max_length` which is \"$1$\" at all indices where the corresponding token is a `self.pad_token` and \"$0$\" elsewhere. Here's an example to illustrate the same:\n","\n","> padded tokens: [\"I\", \"am\", \"Naruto\", \"Uzumaki\", `PAD_TOKEN`, `PAD_TOKEN`] <br/>\n","> input IDs: [2, 3, 4, 1, 0, 0] <br/>\n","> padding mask: [0, 0, 0, 0, 1, 1]\n","\n","You might find [`torch.where`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.where.html) to be helpful here. Upon completion, return a dictionary with `input_ids` and `padding_mask` as keys and associated tensors as values. Let's re-use our example from before and see if everything runs as expected."],"metadata":{"id":"G7csrmTXwNrZ"},"id":"G7csrmTXwNrZ"},{"cell_type":"code","source":["tokenizer = Tokenizer(pad_token=PAD_TOKEN, unk_token=UNK_TOKEN, lowercase=False)\n","tokenizer.from_dict({PAD_TOKEN: 0, UNK_TOKEN: 1, \"I\": 2, \"am\": 3, \"Naruto\": 4})  # stub the tokenizer\n","tokenizer.tokenize(input_seq=\"I am Naruto Uzumaki\")"],"metadata":{"id":"1O6AcZo7zVKX"},"id":"1O6AcZo7zVKX","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Yay! Now that we've completed the tokenization part, we can finally train our own tokenizer using the training data. To train the tokenizer, we will be using the `Tokenizer.train()` class method—observe that this method uses `min_freq` and `remove_frac` to manage the vocabulary size. (We redirect you to the documentation website for more specifics.)\n","\n","The following code cell shows you the effect of `min_freq` and `remove_frac` parameters on the vocabulary size.\n","\n","> __Note.__ Our `Tokenizer.train()` applies `remove_frac` filtering on `min_freq`-filtered output (not the other way around). To simulate this behavior, we use the `min_freq_for_remove_frac` variable below; change this accordingly to see the aggregated effect of applying `remove_frac` over `min_freq` (with `min_freq_for_remove_frac`) filtering."],"metadata":{"id":"5zDdfsE1zUzF"},"id":"5zDdfsE1zUzF"},{"cell_type":"code","source":["min_freq_for_remove_frac = 2  # change this accordingly\n","\n","fig1, ax1 = plt.subplots(1, 1, figsize=(4.5, 3))\n","fig2, ax2 = plt.subplots(1, 1, figsize=(4.5, 3))\n","for split, color in zip([\"train\", \"val\"], [\"lime\", \"b\"]):\n","    text_data = chain(*hf_dataset[split][\"text\"])\n","    token_freqs = Counter(text_data)\n","    freq_vals = list(token_freqs.values())\n","    freq_vals_dict = {min_freq: len([freq for freq in freq_vals if freq >= min_freq]) for min_freq in range(1, 101)}\n","\n","    freq_df = pd.DataFrame({\"min_freq\": freq_vals_dict.keys(), \"vocab size (log scale)\": freq_vals_dict.values()})\n","    sns.lineplot(freq_df, x=\"min_freq\", y=\"vocab size (log scale)\", ax=ax1, label=split, color=color)\n","    ax1.set(yscale=\"log\")\n","    ax1.legend()\n","\n","    freq_vals = [freq for freq in freq_vals if freq >= int(min_freq_for_remove_frac)]\n","    top_tokens = sorted(freq_vals, reverse=True)\n","    top_tokens_dict = {i: len(top_tokens) - int(i * len(top_tokens)) for i in np.arange(0.0, 1.1, 0.1)}\n","    top_tokens_df = pd.DataFrame({\"remove_frac\": top_tokens_dict.keys(), \"vocab size\": top_tokens_dict.values()})\n","    sns.lineplot(top_tokens_df, x=\"remove_frac\", y=\"vocab size\", ax=ax2, label=split, color=color)\n","    ax2.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"xxIdhsi02T5Q"},"id":"xxIdhsi02T5Q","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__Q1.1.1__ In less than three sentences, describe the effect of changing `min_freq` and `remove_frac` on the vocabulary size. How should one go about setting these two hyperparameters? <br/>\n","_Hint. Think about what \"type\" of tokens get removed by increasing the associated hyperparameters._\n","</font>"],"metadata":{"id":"LI2j7PCMRSZN"},"id":"LI2j7PCMRSZN"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"7iAljrO9KS5p"},"id":"7iAljrO9KS5p"},{"cell_type":"markdown","source":["From your criteria above, pick out \"reasonable\" values for `min_freq` and `remove_frac` (this isn't exact science). Finally, let's train our tokenizer on the training data using these chosen hyperparameters. Set these hyperparameter values below and run the following cell to train a tokenizer which will be saved in `CS4740/hw2-fa23/artefacts/tokenizer` folder."],"metadata":{"id":"afbEnShaSzYl"},"id":"afbEnShaSzYl"},{"cell_type":"code","source":["# Change the following two cells with appropriate hyperparameters.\n","min_freq = None\n","remove_frac = None"],"metadata":{"id":"mhoOnVnyULtd"},"id":"mhoOnVnyULtd","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4968320"},"outputs":[],"source":["!train_tokenizer.py \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"train_tokenizer.yml\")} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --filepath-to-store-tokenizer={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --min-freq={min_freq} \\\n","    --remove-frac={remove_frac}"],"id":"a4968320"},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.from_file(os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\"))\n","print(tokenizer)"],"metadata":{"id":"pdoe1bKWUNyQ"},"id":"pdoe1bKWUNyQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"sec12\"></a>\n","#### [1.2] Data collation <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/data_processing/data_collator.py`.\n","\n","Before we begin, it might be worth reading about batching for neural networks; there are excellent blog posts online; for a quick recap, see: https://stats.stackexchange.com/a/153535.\n","\n","Recall that padding is necessary (in most cases) for batching. When it comes to padding, there are two common approaches: static padding and dynamic padding. Static padding pads each sentence in the dataset to the same length, while dynamic padding deals with the data in batches and pads to the longest sentence in a given batch.\n","\n","In this assignment, we will be using dynamic padding instead of static padding. You can run the code cell below to view a distribution of sequence lengths in the train and val data splits."],"metadata":{"id":"O5CLLihuSjS9"},"id":"O5CLLihuSjS9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9062bed8"},"outputs":[],"source":["hf_dataset = hf_dataset.map(lambda data_instance: {\"seq_length\": len(data_instance[\"text\"])}, batched=False)\n","\n","fig, ax = plt.subplots(1, 1, figsize=(4.5, 2))\n","for split, color in zip([\"train\", \"val\"], [\"lime\", \"b\"]):\n","    sns.kdeplot(hf_dataset[split][\"seq_length\"], ax=ax, label=split, color=color)\n","    ax.set(xlabel=\"num tokens\")\n","    ax.legend()\n","plt.tight_layout()\n","plt.show()"],"id":"9062bed8"},{"cell_type":"markdown","source":["---\n","<font color=\"orange\">__Q1.2.1__ Following our data distribution, (in under three sentences,) explain why is dynamic padding better than static padding? <br/>\n","_Hint. Use the plot above to make observations about varying sequence lengths._\n","</font>"],"metadata":{"id":"s1aONep9Vz5D"},"id":"s1aONep9Vz5D"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"zv7lcSIlKZZf"},"id":"zv7lcSIlKZZf"},{"cell_type":"markdown","source":["__Let's dynamically pad!__\n","\n","We will be filling out the `__call__` method of `DataCollator` class in `ner/data_processing/data_collator.py` file (marked with <font color=\"orange\">`TODO-1.2-1`</font>). For those interested, the `__call__` will be similar in function to PyTorch's [`collate_fn`](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn). We want to dynamically pad our batches to the batch max length—we'll use the approach of: 1) see one sample at a time, 2) (if needed,) pad to batch max length, and 3) append the padded tensor to an iterable.\n","\n","First, let's create the iterable in \"append to an iterable\": you can instantiate empty tensors (using [`torch.empty`](https://pytorch.org/docs/stable/generated/torch.empty.html)) to represent the `input_ids` and `padding_mask` for the batch. The dimensions of the empty tensors must be `(batch_size, batch_max_length)` where `batch_size` is the number of sequences in the batch, and `batch_max_length` is the length of the longest sequence in the batch, which can be obtained using `self._get_max_length()`.\n","\n","> __Note.__ When data instances are passed to the `__call__` method, the data instances include \"text\" [= `self.text_colname`], \"index\", and \"NER\" fields for train, val data, while the \"test\" split does _not_ include \"NER\" field [= `self.label_colname`]. We need to handle this!—if the data instance contains `self.label_colname`, then create an additional empty tensor for labels.\n","\n","Next, you can use the tokenizer (accessible via `self.tokenizer`) to tokenize your data. The tokenizer returns `input_ids` and `padding_mask`, which you can store in appropriate empty tensors created before. It is important that you check the shapes of tensors you're appending (you may find [`torch.squeeze`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) helpful to squash any unwanted dimensions).\n","\n","Finally, if the data comes with labels, we'll need to add `PAD_NER_TAG` to the labels wherever the associated token is a padding token. Recall that \"$1$\" in the `padding_mask` represents a padding token; to check if the data is padded is you can use [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html). Here's an example to illustrate the same:\n","\n","> padded tokens: [\"I\", \"am\", \"Naruto\", \"Uzumaki\", `PAD_TOKEN`, `PAD_TOKEN`] <br/>\n","> input IDs: [2, 3, 4, 1, 0, 0] <br/>\n","> padding mask: [0, 0, 0, 0, 1, 1] <br/>\n","> padded labels: [\"O\", \"O\", \"B-PER\", \"I-PER\", `PAD_NER_TAG`, `PAD_NER_TAG`]\n","\n","Similarly, the data might be truncated, in which case, you will have to truncate the labels as well. (Pay special attention to `self.padding_side` and `self.truncation_side`.)\n","\n","Return the results as a dictionary with `input_ids`, `padding_mask`, and `labels` (if present) as keys and associated aggregated tensors as values. Use the cell below to test if your data collator runs as expected; check the shapes of returned tensors (and anything you can think of) are as expected."],"metadata":{"id":"80p6zxEnWUcG"},"id":"80p6zxEnWUcG"},{"cell_type":"code","source":["from ner.data_processing.constants import PAD_NER_TAG\n","from ner.data_processing.data_collator import DataCollator\n","\n","data_instances = [\n","    {\"text\": [\"this\", \"is\", \"hello\"], \"NER\": [\"O\", \"O\", \"B-PER\"]},\n","    {\"text\": [\"the\", \"cycle\", \"is\", \"there\"], \"NER\": [\"O\", \"B-MISC\", \"O\", \"O\"]},\n","]\n","\n","data_collator = DataCollator(\n","    tokenizer=tokenizer,\n","    padding=\"longest\",\n","    max_length=None,\n","    padding_side=\"right\",\n","    truncation_side=\"right\",\n","    pad_tag=PAD_NER_TAG,\n","    text_colname=\"text\",\n","    label_colname=\"NER\",\n",")\n","data_collator(data_instances)\n","# Test shapes of tensors in the returned dictionary."],"metadata":{"id":"nLoP51dLboAl"},"id":"nLoP51dLboAl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"sec2\"></a>\n","### [2] Embeddings <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/nn/embeddings/embedding.py`.\n","\n","Let's first create our `TokenEmbedding` layer using [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html); we can initialize the weights via [`self.apply`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=apply#torch.nn.Module.apply) and using `self.init_weights` provided in `ner/nn/module.py`, or alternatively use [`torch.nn.init`](https://pytorch.org/docs/master/nn.init.html). Fill out <font color=\"orange\">`TODO-2-1`</font> in the `TokenEmbedding.__init__` accordingly.\n","\n","> __Tip.__ Notice how all the neural models inherit `ner.nn.module.Module` (note the `super().__init__()` line); it is worth spending time viewing the functionality provided within `ner.nn.module.Module`. One such functionality suggested above is `init_weights()`.\n","\n","We've gone ahead and created a test embedding for you below, you can test that the shape of the embedding component is as expected. (Another useful `ner.nn.module.Module` method we use below is `print_params()` to view the model parameters.)"],"metadata":{"id":"sTXlLx1WcbeD"},"id":"sTXlLx1WcbeD"},{"cell_type":"code","source":["from ner.data_processing.constants import PAD_TOKEN\n","from ner.nn.embeddings.embedding import TokenEmbedding\n","\n","token_embedding = TokenEmbedding(vocab_size=2, embedding_dim=10, padding_idx=tokenizer.token2id[PAD_TOKEN])\n","token_embedding.print_params()  # see the model parameters\n","# Test shapes of token_embedding.* components."],"metadata":{"id":"kE8Li1Csg6ZE"},"id":"kE8Li1Csg6ZE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we've initialized our `TokenEmbedding`, let's go ahead and fill in the `forward()` part under <font color=\"orange\">`TODO-2-2`</font>. This takes in the tokenized `input_ids` of `(batch_size, batch_max_length)` and creates embeddings of shape: `(batch_size, batch_max_length, embedding_dim)` out of them, by passing them through the embedding layer. Make sure the dimensions of your input and output tensors are as expected.\n","\n","Upon completion, you can run the cell below to confirm everything is as expected."],"metadata":{"id":"O5CG-0k0iLSA"},"id":"O5CG-0k0iLSA"},{"cell_type":"code","source":["vocab_size = 2\n","batch_size, batch_max_length = 4, 6\n","\n","token_embedding = TokenEmbedding(vocab_size=2, embedding_dim=10, padding_idx=tokenizer.token2id[PAD_TOKEN])\n","input_embeddings = token_embedding(input_ids=torch.randint(0, vocab_size, (batch_size, batch_max_length)))\n","input_embeddings.shape  # check if the shape matches to that intended"],"metadata":{"id":"tjHJwP4NiqLd"},"id":"tjHJwP4NiqLd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","<a name=\"sec3\"></a>\n","### [3] The training, evaluation loop <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/trainers/trainer.py`.</font>\n","\n","In this section, we will implement functionality to train and evaluate any neural network [= FFNN, RNN in our case]. Before we begin, observe the loss function used in `Trainer.__init__` to be [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n"],"metadata":{"id":"AAVa6TcZjaoN"},"id":"AAVa6TcZjaoN"},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__Q3.1.__ What is the difference between [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) and [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)? What implication does using `nn.CrossEntropyLoss` have on the use of [`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) before passing the logits to `nn.CrossEntropyLoss`? (Explain in under three sentences.)\n","</font>"],"metadata":{"id":"7VK3gR3Mm6B6"},"id":"7VK3gR3Mm6B6"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"kfjQUaDVKhaR"},"id":"kfjQUaDVKhaR"},{"cell_type":"markdown","source":["__Training__\n","\n","Let's implement the `_train_epoch()` method in the `Trainer` class (fill out <font color=\"orange\">`TODO-3-1`</font>). This is a standard training loop, where a batch consists of `input_ids`, `padding_mask`, and `labels` processed by the `DataCollator`.\n","\n","> __Tip.__ In PyTorch, all tensors are expected to be on the same device, this means that the `self.model` and `input_ids` must be on the same device. You can use `x.to(self.device)` in the `Trainer` class to move a tensor `x` to a desired device.\n","\n","Before we start, set the model to `train` mode (why?). We'll loop over batches of the dataloader, and for each batch:\n","* zero-out the gradient,\n","* noting that the input to a model is just `input_ids`, get predictions from a given model (use `self.model(input_ids)`; use appropriate device mapping),\n","* compute the loss using the model predictions and labels; you can use `compute_loss()` from `ner.utils.metrics` (the function is already imported in the `Trainer` file),\n","* backpropagate the loss,\n","* drop all intermediate buffers that are unwanted by calling [`.item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) on the loss—we leave it as a self-exercise for you to explore why this step is needed,\n","* [clip the gradient norm](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) using `self.grad_clip_max_norm` as the norm value; you can read about the implications of \"exploding\" gradients,\n","* update the model parameters, and\n","* compute batch-level metrics (we will average metrics across all batches to compute the overall performance).\n","\n","(For the last step above,) use the following code in `_train_epoch()` to generate appropriate metrics for the current batch, and to append the computed metrics to the `metrics` local variable (dictionary):\n","\n","```python\n","# Compute metrics for a given batch.\n","batch_metrics = compute_metrics(\n","    preds=preds,\n","    labels=batch[\"labels\"],\n","    padding_mask=batch[\"padding_mask\"],\n","    other_ner_tag_idx=self.other_ner_tag_idx,\n","    average=\"weighted\",\n",")\n","# Append batch-level metrics to `metrics` local variable.\n","for key in metrics:\n","    metrics[key].append(batch_metrics[key]) if key != \"loss\" else metrics[key].append(loss)\n","```\n","\n","Let's test to see if everything runs as expected (we'll use the previously created `TokenEmbedding` as a stub for our \"model\"). Don't worry about the performance, it's just to ensure that everything \"runs\".\n"],"metadata":{"id":"PrJx41-Fnwnl"},"id":"PrJx41-Fnwnl"},{"cell_type":"code","source":["from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","\n","from ner.data_processing.constants import NER_ENCODING_MAP\n","from ner.data_processing.constants import PAD_TOKEN\n","from ner.trainers.trainer import Trainer\n","\n","data_instances = datasets.Dataset.from_dict(\n","    {\n","        \"text\": [[\"Hello\", \"my\", \"friend\"], [\"My\", \"name\", \"is\", \"Naruto\", \"Uzumaki\"], [\"I\", \"am\", \"Shisui\"]],\n","        \"NER\": [[\"O\", \"O\", \"O\"], [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\"], [\"O\", \"O\", \"B-PER\"]],\n","        \"index\": [[19, 20, 21], [2, 3, 4, 5, 6], [11, 12, 13]],\n","    }\n",")\n","data_collator = DataCollator(\n","    tokenizer=tokenizer,\n","    padding=\"longest\",\n","    max_length=None,\n","    padding_side=\"right\",\n","    truncation_side=\"right\",\n","    pad_tag=PAD_NER_TAG,\n","    text_colname=\"text\",\n","    label_colname=\"NER\",\n",")\n","dataloader = DataLoader(data_instances, collate_fn=data_collator, batch_size=1)\n","\n","token_embedding = TokenEmbedding(\n","    vocab_size=tokenizer.vocab_size,\n","    embedding_dim=(len(NER_ENCODING_MAP.keys()) - 1),\n","    padding_idx=tokenizer.token2id[PAD_TOKEN],\n",")\n","\n","optimizer = AdamW(token_embedding.parameters())\n","trainer = Trainer(model=token_embedding, optimizer=optimizer, data_collator=data_collator, train_data=data_instances)\n","trainer._train_epoch(dataloader)"],"metadata":{"id":"y5DOrfC4uudP"},"id":"y5DOrfC4uudP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__Evaluation__\n","\n","Now let's implement the `_eval_epoch()` method in the `Trainer` class (fill out <font color=\"orange\">`TODO-3-2`</font>). This is a standard evaluation loop, where a batch consists of `input_ids`, `padding_mask`, and `labels` processed by the `DataCollator`."],"metadata":{"id":"PUX2qqKVsuG2"},"id":"PUX2qqKVsuG2"},{"cell_type":"markdown","source":["---\n","<font color=\"orange\">__Q3.2.__ Before proceeding, observe the `@torch.no_grad()` annotation on `Trainer._eval_epoch()`—why are we using \"no grad\" at evaluation? (Explain in under three sentences.)\n","</font>"],"metadata":{"id":"OGlWHF5pyizB"},"id":"OGlWHF5pyizB"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"2-UbqHtwKkR4"},"id":"2-UbqHtwKkR4"},{"cell_type":"markdown","source":["Before we implement `_eval_epoch()`, set the model to `eval` mode (why?). We'll loop over batches of the dataloader, and for each batch:\n","* noting that the input to a model is just `input_ids`, get predictions from a given model (use `self.model(input_ids)`; use appropriate device mapping),\n","* compute the loss using the model predictions and labels; you can use `compute_loss()` from `ner.utils.metrics` (the function is already imported in the `Trainer` file),\n","* use [`.item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) on the loss to drop all intermediate buffers, and\n","* compute batch-level metrics (we will average metrics across all batches to compute the overall performance).\n","\n","You can reuse the code above to generate appropriate metrics for the current batch, and to append the computed metrics to the `metrics` local variable (dictionary). Let's use the same stubbing as before to ensure everything runs as expected.\n","\n","> __Tip.__ An easy thing to check (and note) is that evaluation should take lesser time than training!"],"metadata":{"id":"ecUiNltIy-J0"},"id":"ecUiNltIy-J0"},{"cell_type":"code","source":["from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","\n","from ner.data_processing.constants import NER_ENCODING_MAP\n","from ner.data_processing.constants import PAD_TOKEN\n","from ner.trainers.trainer import Trainer\n","\n","data_instances = datasets.Dataset.from_dict(\n","    {\n","        \"text\": [[\"Hello\", \"my\", \"friend\"], [\"My\", \"name\", \"is\", \"Naruto\", \"Uzumaki\"], [\"I\", \"am\", \"Shisui\"]],\n","        \"NER\": [[\"O\", \"O\", \"O\"], [\"O\", \"O\", \"O\", \"B-PER\", \"I-PER\"], [\"O\", \"O\", \"B-PER\"]],\n","        \"index\": [[19, 20, 21], [2, 3, 4, 5, 6], [11, 12, 13]],\n","    }\n",")\n","data_collator = DataCollator(\n","    tokenizer=tokenizer,\n","    padding=\"longest\",\n","    max_length=None,\n","    padding_side=\"right\",\n","    truncation_side=\"right\",\n","    pad_tag=PAD_NER_TAG,\n","    text_colname=\"text\",\n","    label_colname=\"NER\",\n",")\n","dataloader = DataLoader(data_instances, collate_fn=data_collator, batch_size=1)\n","\n","token_embedding = TokenEmbedding(\n","    vocab_size=tokenizer.vocab_size,\n","    embedding_dim=(len(NER_ENCODING_MAP.keys()) - 1),\n","    padding_idx=tokenizer.token2id[PAD_TOKEN],\n",")\n","\n","optimizer = AdamW(token_embedding.parameters())\n","trainer = Trainer(model=token_embedding, optimizer=optimizer, data_collator=data_collator, train_data=data_instances)\n","trainer._eval_epoch(dataloader)"],"metadata":{"id":"57spKHdzz0_J"},"id":"57spKHdzz0_J","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","<a name=\"milestone\"></a>\n","### [$\\ast$] Milestone submission <small>[↩︎](#outline)</small>\n","\n","Run the following cell to make a milestone submission. The `make_submission` command when run with `--milestone-submission` flag creates a `milestone_submission.zip` file in `CS4740/hw2-fa23/artefacts` folder, which is to be submitted on the submission site(s).  <font color=\"red\">Caution: the script will overwrite any file named `milestone_submission.zip` existing in `CS4740/hw2-fa23/artefacts` folder.</font>\n","\n","The `milestone_submission.zip` is all that you will need to submit for the milestone (no need to submit anything else!)."],"metadata":{"id":"JZp865q71_xr"},"id":"JZp865q71_xr"},{"cell_type":"code","source":["submission_filepath = f\"{ARTEFACTS_DIR}\"\n","\n","!make_submission.py \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --basepath-to-store-submission={os.path.join(ARTEFACTS_DIR, submission_filepath)} \\\n","    --net-ids={net_ids} \\\n","    --milestone-submission\n","\n","if os.path.isfile(f\"{os.path.join(ARTEFACTS_DIR, 'milestone_submission.zip')}\"):\n","    display(success())\n","else:\n","    print(colored(\"Oops, something went wrong!\", \"red\"))"],"metadata":{"id":"t7GKaRwp2kkL"},"id":"t7GKaRwp2kkL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","<a name=\"sec4\"></a>\n","### [4] FFNNs <small>[↩︎](#outline)</small>\n","\n","In this section, you will implement and train a Feed Forward Neural Network (FFNN) to classify tokens into appropriate named-entities. (Contrasting FFNN to an RNN,) FFNNs are _parallel_ processors—they can process an entire sequence in one go! Let's look at how an FFNN processes a single token:\n","\n","<div align=\"center\">\n","    <img src=\"https://i.imgur.com/jERkG7r.png\" width=\"240\"/>\n","    <br/>\n","    <b>Fig. 1.</b> Forward pass of a single token through a single hidden-layer [= one pink layer in the middle!] FFNN\n","    <br/>\n","    $x_k^{(j)}$, $z_k^{(i)}$, $y_k^{(l)}$ indicate the $j$-th embedding, $i$-th hidden, and $l$-th output dimensions associated with the $k$-th input token\n","    <br/>\n","    (Adapted from: <a href=\"https://tikz.net/neural_networks/\">TikZ.net</a>)\n","</div>\n","\n","The FFNN processes a sequence of $L$ tokens, where each token $x_k \\in \\mathbb{R}^d$ gets mapped to the hidden dimension $\\mathbb{R}^h$ through a linear transformation:\n","$$\n","z_k = f(W x_k + b_W),\n","$$\n","\n","where $W \\in \\mathbb{R}^{h \\times d}$ and $b_W \\in \\mathbb{R}^h$ are the weights and biases associated with the transformation, $z_k \\in \\mathbb{R}^h$ is the hidden intermediate, and $f(\\cdot)$ is some nonlinearity applied element-wise. In practice, these computations are all \"matrixified\":\n","$$\n","Z = f(\\mathrm{X} \\mathrm{W}^T) \\equiv f(Z'),\n","$$\n","\n","where $Z \\in \\mathbb{R}^{L \\times h}$, $\\mathrm{X} \\in \\mathbb{R}^{L \\times d+1}$ (often known as the design matrix) is the matrix of input token embeddings with a column of ones appended at the end, and $\\mathrm{W} \\in \\mathbb{R}^{h \\times d+1}$ (upright-$\\mathrm{W}$, not slanted-$W$) is the weight matrix with bias vector absorbed as the last column of the matrix. $\\mathrm{W}$ is learned through backpropagation [= the training loop in [section 3](#sec3)].\n","\n","> __Notation.__ From hereon, we use the notation \"upright-$\\mathrm{M}$\" (or simply \"$\\mathrm{M}$\") to indicate that 1) if (slanted-)$M$ is a _weight_ matrix, then the bias is absorbed into the matrix, or 2) if (slanted-)$M$ is a _design_ matrix, then a column of ones has been appended to the matrix.\n","\n","A similar reasoning follows the hidden-to-output mapping, resulting in:\n","$$\n","Y = g(\\mathrm{Z} \\mathrm{V}^T) \\equiv g(Y'),\n","$$\n","where $Y \\in \\mathbb{R}^{L \\times o}$, upright-$\\mathrm{Z} \\in \\mathbb{R}^{L \\times h+1}$, and upright-$\\mathrm{V} \\in \\mathbb{R}^{o \\times h+1}$, $g(\\cdot)$ is an element-wise nonlinearity, which is a softmax function for classification problems. Also, the row vectors of $Y'$ are what is often referred to as \"_unnormalized_ logits\", i.e., predictions before they are passed through a softmax/sigmoid.\n","\n","> __Food for thought.__ What happens if we have an FFNN without any hidden layers, and one output layer with softmax activation?—where have you seen this before? <br/>\n","> (This is an ungraded question, it is meant to get you thinking about the origin of FFNNs.)\n","\n","One final note: A \"quirk\" of PyTorch is that you never have to explicitly form upright-$\\mathrm{X}$ (or upright-$\\mathrm{Z}$); PyTorch does this for you! For example, an `nn.Linear` (equivalent to upright-$\\mathrm{M}$) transforms slanted-$A$ (with some dimensions) into slanted-$B$ of appropriate dimensions."],"metadata":{"id":"BcKxhXnP0GFj"},"id":"BcKxhXnP0GFj"},{"cell_type":"markdown","source":["<a name=\"sec41\"></a>\n","\n","#### [4.1] Single-layer FFNN <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/nn/models/ffnn.py`.</font>\n","\n","In this section, we will implement a simple, single-layer FFNN. For this part of the implementation, we will \"forget\" about the existence of `num_layers` (for convenience, we set `num_layers = 1` in the FFNN class constructor).\n","\n","__Initializing the FFNN__\n","\n","Let's implement the `__init__` of our `FFNN` class based on how FFNNs are mathematically represented. To this end,\n","* we need $\\mathrm{W}$, $\\mathrm{V}$, each implemented as [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) such that $\\mathrm{W}: X$ → $Z$ and $\\mathrm{V}: Z$ → $Y$—this needs to be filled in by you, under <font color=\"orange\">`TODO-4-1`</font>,\n","* a nonlinearity $f(\\cdot)$—we'll be using [nonlinearities available in `torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions) (specifically, `F.relu`) and directly applying them as needed when we implement the `forward()` part—let's ignore [= don't include it in `__init__`] this part for now!, and\n","* weight initialization of the transformation matrices: already implemented using `self.apply(self.init_weights)`.\n","\n","Feel free to test that your initialization runs as expected in the cell below—we've gone ahead and created a test FFNN for you, you can check if the shapes of the model components are as expected."],"metadata":{"id":"eEiOfQ_w4uvK"},"id":"eEiOfQ_w4uvK"},{"cell_type":"code","source":["from ner.nn.models.ffnn import FFNN\n","\n","ffnn = FFNN(embedding_dim=10, hidden_dim=5, output_dim=2)\n","# Test shapes of ffnn.* components."],"metadata":{"id":"SOAcUjkK825L"},"id":"SOAcUjkK825L","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__The forward pass__\n","\n","Now that we've successfully initialized the FFNN, let's try to run a `forward()` pass through the network. What does this entail?:\n","* using $\\mathrm{W}$, transform the input $X$ of shape `(batch_size, batch_max_length, embedding_dim)` to $Z' = \\mathrm{XW}^T$, which is the hidden intermediate,\n","* apply a nonlinearity [`F.relu`](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu) on the hidden intermediate (don't use any other nonlinearity), and\n","* using $\\mathrm{V}$, transform the hidden intermediate $Z$ to $Y' = \\mathrm{ZV}^T$ of shape `(batch_size, batch_max_length, output_dim)`.\n","\n","We would normally apply a softmax over $Y'$ such that $Y = \\text{softmax}(Y')$; however, revisit Q3.1 to note if this is needed. With this in mind, fill out the <font color=\"orange\">`TODO-4-2`</font> part in the `forward()` method to run a forward pass.\n","\n","Upon completion, you can run the cell below to train your FFNN! Change the `batch_size` and `num_epochs` below as you see fit; all other hyperparameters (e.g., `embedding_dim`, `hidden_dim`, etc.) are present in `scripts/configs/train_model.yml`—you're free to change these as well. Again, don't worry too much about the performance—the following is just a test run. (The model artefacts are stored under `<experiment-name>` subfolder of `CS4740/hw2-fa23/artefacts/experiments` folder.)\n","\n","> __Training time.__ With a batch size of $128$, training a single-layered FFNN for one epoch on a Colab CPU takes about ~$6$mins, while on a Colab T4 GPU it takes ~$1.5$mins. Owing to hardware limitations, we do not recommend increasing the batch size beyond $128$."],"metadata":{"id":"xzeS_OLn-OwV"},"id":"xzeS_OLn-OwV"},{"cell_type":"code","source":["# Set the batch size and number of training epochs.\n","batch_size = None\n","num_epochs = None"],"metadata":{"id":"thWc9hitDfde"},"id":"thWc9hitDfde","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_type = \"ffnn\"\n","num_layers = 1\n","\n","experiment_name = f\"model={model_type}_layers={num_layers}_batch={batch_size}\"\n","\n","!train_model.py \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"train_model.yml\")} \\\n","    --tokenizer-config-path={os.path.join(CONFIGS_DIR, \"train_tokenizer.yml\")} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --model-type={model_type} \\\n","    --num-layers={num_layers} \\\n","    --batch-size={batch_size} \\\n","    --num-epochs={num_epochs} \\\n","    --basepath-to-store-results={os.path.join(ARTEFACTS_DIR, \"experiments\")} \\\n","    --experiment-name={experiment_name}"],"metadata":{"id":"8YqmvSoc9ZPi"},"id":"8YqmvSoc9ZPi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the saved checkpoints, let's load the best model (if your `num_epochs` was more than one, else the best model is the only model) based on the `entity_f1` validation performance logged above; set the `best_epoch` below to reflect the epoch (zero-indexed) that resulted in the best model.\n","\n","> __Tip.__ Check out the files stored in the `CS4740/hw2-fa23/artefacts/experiments` folder; there are quite a few important files stored there, including the training metrics, validation metrics, model checkpoints (one per epoch), etc."],"metadata":{"id":"G68diKs7ERQu"},"id":"G68diKs7ERQu"},{"cell_type":"code","source":["# Change the best epoch value.\n","best_epoch = None"],"metadata":{"id":"6fSH0ns-GZfx"},"id":"6fSH0ns-GZfx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["config_path = os.path.join(ARTEFACTS_DIR, f\"experiments/{experiment_name}/config.json\")\n","with open(config_path, \"r\") as fp:\n","    config = yaml.safe_load(fp)\n","\n","checkpoint_filename = f\"experiments/{experiment_name}/checkpoints/checkpoint_{best_epoch}.ckpt\"\n","model = NERPredictor(\n","    vocab_size=tokenizer.vocab_size,\n","    padding_idx=tokenizer.token2id[tokenizer.pad_token],\n","    output_dim=len(NER_ENCODING_MAP) - 1,\n","    **config[\"model\"],\n",")\n","checkpoint = torch.load(os.path.join(ARTEFACTS_DIR, checkpoint_filename), map_location=torch.device(\"cpu\"))\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.print_params()"],"metadata":{"id":"ZuO6VmFQEEiV"},"id":"ZuO6VmFQEEiV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how well our single-layer learned model performs on unseen data; running the cell below shows the model's predictions for a chosen sample (change the `sample_idx` value to retrieve a different sample)."],"metadata":{"id":"FFyiLsYWDyta"},"id":"FFyiLsYWDyta"},{"cell_type":"code","source":["partition, sample_idx = \"val\", 3\n","labels = hf_dataset[partition][sample_idx][\"NER\"] if \"NER\" in hf_dataset[partition][sample_idx] else None\n","_ = inspect_preds(tokenizer=tokenizer, model=model, text=hf_dataset[partition][sample_idx][\"text\"], labels=labels)"],"metadata":{"id":"drSpyL8QD7oB"},"id":"drSpyL8QD7oB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"sec42\"></a>\n","\n","#### [4.2] Multi-layered FFNN <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/nn/models/ffnn.py` (same file used in [section 4.1](#sec41)).</font>\n","\n","Yay! Congrats on running your first FFNN! Now, let's go back and adapt our single-layered FFNN into a multi-layered FFNN. <font color=\"red\">This is the implementation that you'll be submitting to us (not the single-layered implementation).</font>\n","\n","__Accommodating multiple layers at initialization__\n","\n","In our first attempt, we initialized such that we project to and from a single hidden layer; we now wish to support an arbitrary number of hidden layers corresponding to `num_layers`. Let's modify the `__init__` of our `FFNN` class to accommodate this. How?:\n","* we still need $\\mathrm{W}: X$ → $Z_1$ ($Z_1$ indicates the first hidden layer), so let's retain $\\mathrm{W}$,\n","* we also need $\\mathrm{V}: Z_n$ → $Y$ ($Z_n$ indicates the last hidden layer), so let's also retain $\\mathrm{V}$,\n","* when `num_layers` is greater than one, we need an appropriate number of weight matrices, $\\mathrm{U}_k$s, such that $\\mathrm{U}_k: Z_k$ → $Z_{k+1}$; these are mappings from hidden dimension $\\mathbb{R}^h$ to hidden dimension $\\mathbb{R}^h$—each $\\mathrm{U}_k$ can be implemented as [`nn.Linear`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Linear.html) and the list of all $\\mathrm{U}_k$s can be maintained using an [`nn.ModuleList`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.ModuleList.html) (why is an `nn.ModuleList` used instead of just maintaining a list of `nn.Linear`s?; we leave this as a self-exercise).\n","\n","With these changes, your FFNN should initialize one input layer, one output layer, and `num_layers`-many hidden layers. Upon completion, run the cell below to see if the model parameters are as expected."],"metadata":{"id":"N-CjI8PVK_4B"},"id":"N-CjI8PVK_4B"},{"cell_type":"code","source":["ffnn = FFNN(embedding_dim=10, hidden_dim=5, output_dim=2, num_layers=5)\n","ffnn.print_params()"],"metadata":{"id":"CaMf7_IKaQQZ"},"id":"CaMf7_IKaQQZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__The forward pass, take-2!__\n","\n","Now that we have successfully initialized our FFNN to support multiple layers, we need to update the FFNN's `forward()` method to forward propagate through _all_ of the hidden layers (not just the first one!). What does this look like?:\n","* using $\\mathrm{W}$, transform the input $X$ to $Z'_1 = \\mathrm{XW}^T$, which is the first hidden intermediate, and applying a nonlinearity [`F.relu`](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu) on the first hidden intermediate—we've already completed this in [section 4.1](#sec41)!,\n","* for each $\\mathrm{U}_k$ ($k > 1$), we need to compute $Z'_{k} = \\mathrm{Z}_{k-1} \\mathrm{U}_k^T$ and apply [`F.relu`](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu) on the obtained $Z_k'$—make changes to include this functionality, and finally\n","* using $\\mathrm{V}$, transform the last ($n$-th) hidden intermediate $Z_n$ to $Y' = \\mathrm{Z}_n\\mathrm{V}^T$—modify the existing code to reflect this.\n","\n","Again, should we apply a softmax over $Y'$ such that $Y = \\text{softmax}(Y')$?\n","\n","Upon completion, you can run the cell below to train your multi-layered FFNN! Change the `num_layers`, `batch_size`, and `num_epochs` below as you see fit; all other hyperparameters (e.g., `embedding_dim`, `hidden_dim`, etc.) are present in `scripts/configs/train_model.yml`—you're free to change these as well. (The model artefacts are stored under `<experiment-name>` subfolder of `CS4740/hw2-fa23/artefacts/experiments` folder.)\n","\n","> __Training time.__ With a batch size of $128$, training a two-layered FFNN for one epoch on a Colab CPU takes about ~$10$mins, while on a Colab T4 GPU it takes ~$2$mins. Again, owing to hardware limitations, we do not recommend increasing the batch size beyond $128$.\n","\n","<font color=\"red\">Do not use `num_layers` greater than $2$; using more than two layers causes unwanted out-of-memory errors on our autograder servers.</font> (You are free to experiment with more than two layers, but the models in the final submission __cannot__ have more than two layers.)"],"metadata":{"id":"UvBzb-izai3g"},"id":"UvBzb-izai3g"},{"cell_type":"code","source":["# Set the number of layers, batch size, and number of training epochs.\n","num_layers = None\n","batch_size = None\n","num_epochs = None"],"metadata":{"id":"XtKNpR5jaHxZ"},"id":"XtKNpR5jaHxZ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b31f614d","scrolled":false},"outputs":[],"source":["model_type = \"ffnn\"\n","\n","experiment_name = f\"model={model_type}_layers={num_layers}_batch={batch_size}\"\n","\n","!train_model.py \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"train_model.yml\")} \\\n","    --tokenizer-config-path={os.path.join(CONFIGS_DIR, \"train_tokenizer.yml\")} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --model-type={model_type} \\\n","    --num-layers={num_layers} \\\n","    --batch-size={batch_size} \\\n","    --num-epochs={num_epochs} \\\n","    --basepath-to-store-results={os.path.join(ARTEFACTS_DIR, \"experiments\")} \\\n","    --experiment-name={experiment_name}"],"id":"b31f614d"},{"cell_type":"markdown","source":["Just as before, from the saved checkpoints, let's load the best model (if your `num_epochs` was more than one, else the best model is the only model) based on the `entity_f1` validation performance logged above; set the `best_epoch` below to reflect the epoch (zero-indexed) that resulted in the best model.\n","\n","> __Tip.__ Think you should've run for more epochs? Fret not, we've got you!—our `train_model.py` script has a parameter `--pretrained-checkpoint-or-model-filepath` that let's you use a pretrained checkpoint .ckpt or pretrained model .pt and continue training. (Check out the documentation website for more specifics.)"],"metadata":{"id":"kjWoEr-DnaXQ"},"id":"kjWoEr-DnaXQ"},{"cell_type":"code","source":["# Change the best epoch value.\n","best_epoch = None"],"metadata":{"id":"KjUjV76anrpJ"},"id":"KjUjV76anrpJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["config_path = os.path.join(ARTEFACTS_DIR, f\"experiments/{experiment_name}/config.json\")\n","with open(config_path, \"r\") as fp:\n","    config = yaml.safe_load(fp)\n","\n","checkpoint_filename = f\"experiments/{experiment_name}/checkpoints/checkpoint_{best_epoch}.ckpt\"\n","model = NERPredictor(\n","    vocab_size=tokenizer.vocab_size,\n","    padding_idx=tokenizer.token2id[tokenizer.pad_token],\n","    output_dim=len(NER_ENCODING_MAP) - 1,\n","    **config[\"model\"],\n",")\n","checkpoint = torch.load(os.path.join(ARTEFACTS_DIR, checkpoint_filename), map_location=torch.device(\"cpu\"))\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.print_params()"],"metadata":{"id":"JugM_uFvnavf"},"id":"JugM_uFvnavf","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's analyze how well our multi-layer learned model performs on unseen data; running the cell below shows the model's predictions for a chosen sample (change the `sample_idx` value to retrieve a different sample)."],"metadata":{"id":"_7hGwgbjoYCO"},"id":"_7hGwgbjoYCO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f75d6697"},"outputs":[],"source":["partition, sample_idx = \"val\", 3\n","labels = hf_dataset[partition][sample_idx][\"NER\"] if \"NER\" in hf_dataset[partition][sample_idx] else None\n","_ = inspect_preds(tokenizer=tokenizer, model=model, text=hf_dataset[partition][sample_idx][\"text\"], labels=labels)"],"id":"f75d6697"},{"cell_type":"markdown","source":["Finally, it's often important to understand what the model is looking at when it makes a specific prediction. The easiest way to do this is by looking at the activation values [= outputs from `F.relu`] and see if there are any specific patterns that the model is learning. This is a fairly well-researched area of NLP (and CV), and is often tagged with the keywords: \"interpretability\" and \"explainability\".\n","\n","Set the `module` variable below to visualize the activations of a specific module within your model—use the syntax: `<model-varname>.model.<module-name>` (e.g., `model.model.W` to visualize a layer named `W` in your FFNN named `model`)."],"metadata":{"id":"Anaag3aTp-Cx"},"id":"Anaag3aTp-Cx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1961a87e"},"outputs":[],"source":["# Set the module you wish to visualize (format: model.model.<module-name>).\n","module = None\n","\n","visualize_activations(\n","    tokenizer=tokenizer,\n","    model=model,\n","    module=module,\n","    text=hf_dataset[partition][sample_idx][\"text\"],\n","    labels=labels,\n","    nonlinearity=F.relu,\n",")"],"id":"1961a87e"},{"cell_type":"markdown","source":["<a name=\"leaderboard_ffnn\"></a>\n","\n","#### [$\\ast$] Leaderboard submission <small>[↩︎](#outline)</small>\n","\n","> __Note.__ Submitting to the leaderboard is optional, see [<sup>[2]</sup>](#optional) for baselines and related information.\n","\n","Now that we've trained our FFNN, we can go ahead and make a leaderboard submission. Run the following cells to make a leaderboard submission. The `make_submission` command when run with `--leaderboard-submission` flag creates a `leaderboard_submission.zip` file in `CS4740/hw2-fa23/artefacts` folder, which is to be submitted on the submission site. <font color=\"red\">Caution: the script will overwrite any file named `leaderboard_submission.zip` existing in `CS4740/hw2-fa23/artefacts` folder.</font>\n","\n","Set the `ffnn_experiment_name` and `ffnn_best_epoch` accordingly. The `leaderboard_submission.zip` is all that you will need to submit to the leaderboard (no need to submit anything else!)."],"metadata":{"id":"KNC8B-mZw7cK"},"id":"KNC8B-mZw7cK"},{"cell_type":"code","source":["# Set the following accordingly.\n","ffnn_experiment_name = None\n","ffnn_best_epoch = None"],"metadata":{"id":"cl3k7a-Rxs3F"},"id":"cl3k7a-Rxs3F","execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_filepath = f\"{ARTEFACTS_DIR}\"\n","\n","ffnn_config_filename =  f\"experiments/{ffnn_experiment_name}/config.json\"\n","ffnn_checkpoint_filename = f\"experiments/{ffnn_experiment_name}/checkpoints/checkpoint_{ffnn_best_epoch}.ckpt\"\n","\n","!make_submission.py \\\n","    --ffnn-config-path={os.path.join(ARTEFACTS_DIR, ffnn_config_filename)} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --basepath-to-store-submission={os.path.join(ARTEFACTS_DIR, submission_filepath)} \\\n","    --pretrained-ffnn-checkpoint-or-model-filepath={os.path.join(ARTEFACTS_DIR, ffnn_checkpoint_filename)} \\\n","    --leaderboard-submission\n","\n","if os.path.isfile(f\"{os.path.join(ARTEFACTS_DIR, 'leaderboard_submission.zip')}\"):\n","    display(success())\n","else:\n","    print(colored(\"Oops, something went wrong!\", \"red\"))"],"metadata":{"id":"wa3Va-Idw6dQ"},"id":"wa3Va-Idw6dQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"sec43\"></a>\n","\n","#### [4.3] Analyzing FFNN <small>[↩︎](#outline)</small>\n","\n","Please be concise when answer the following questions; brevity is your friend! These questions are to get you thinking about how FFNNs are often designed/experimented with, and are _not_ meant to be an unordered collection of all your thoughts about FFNNs. Make compelling arguments (preferably those that are backed by data) that aren't misleading or confusing.\n","\n","(All the questions in this section must be answered in under two pages.)"],"metadata":{"id":"WplApcjqyfIL"},"id":"WplApcjqyfIL"},{"cell_type":"markdown","source":["---\n","<font color=\"orange\">__Q4.3.1.__ In comparison to HMM and MEMM models from the previous assignment, how did your best FFNN model _perform_? Performance is more than just \"performance on some metric\"; it includes efficiency (e.g., training time), memory (e.g., weights storage), generalizability (e.g., on unknown words), etc. Choose any two dimensions and present your views.\n","</font>"],"metadata":{"id":"lmth4GgC1O--"},"id":"lmth4GgC1O--"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"uzaepWI-2c39"},"id":"uzaepWI-2c39"},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__Q4.3.2.__ We assume that you experimented with some (if not all) hyperparameters. Can you comment on some patterns you observed in hyperparameter tuning—e.g., variations in performance with batch size, number of layers, hidden dimension, embedding dimension, etc. Choose any two hyperparameters.<br/>\n","(Don't worry!, all your experiments and related metrics are stored in the `CS4740/hw2-fa23/artefacts/experiments` folder.)\n","</font>"],"metadata":{"id":"34GWDpgV28Ji"},"id":"34GWDpgV28Ji"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"IHXmPNJZ4JBr"},"id":"IHXmPNJZ4JBr"},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__Q4.3.3.__ We provide functionality to visualize activations of the modules of your trained FFNN. See how activations for named-entities vary (when compared to non named-entities) as you pass through the layers of a multi-layered FFNN. Do they get sparser as you get deeper into the network? Or is it the other way around? Maybe there's no specific pattern? <br/>\n","Don't look for one example where some named entity has a specific pattern and base your answer on that; look for any interesting and general patterns.\n","</font>"],"metadata":{"id":"h9C18un42gnK"},"id":"h9C18un42gnK"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"QVHj2kQc6UHv"},"id":"QVHj2kQc6UHv"},{"cell_type":"markdown","source":["---\n","<a name=\"sec5\"></a>\n","\n","### [5] RNNs, or \"[multilayer machines with loops!](https://direct.mit.edu/books/book/3132/chapter-abstract/85850/Perceptrons-and-Pattern-Recognition)\" <small>[↩︎](#outline)</small>\n","\n","Recurrent Neural Networks (RNNs) are _sequential_ data processors (contrasting to FFNNs, which are parallel processors), capable of retaining information over time [= often thought of as the \"memory\" of an RNN]. So what does an RNN look like?:\n","\n","<div align=\"center\">\n","    <img src=\"https://i.imgur.com/omdZzRr.png\" width=\"450\"/>\n","    <br/>\n","    <b>Fig. 2.</b> Compressed (left) and unfolded (right) single-layered RNN\n","    <br/>\n","</div>\n","\n","where $x_t$, $z_t$, and $o_t$ are the input, hidden state, and output at timestep $t$. The recurrent nature of RNNs give us the following \"nice\" properties:\n","* the length of the inputs and outputs can be varied (unlike with FFNNs)—this is axiomatically important for language tasks,\n","* at each timestep $t$, a hidden state $z_t$ maintains a memory of the past and present information [= context], using the previous hidden state $z_{t-1}$ and the input $x_t$, and\n","* weights $\\mathrm{W}$, $\\mathrm{U}$, $\\mathrm{V}$ are shared across all timesteps!\n","\n","The RNN above processes a sequence of $L$ tokens [= timesteps] $x_t$s; for input $x_t \\in \\mathbb{R}^d$, hidden state $z_t \\in \\mathbb{R}^h$, and output state $o_t \\in \\mathbb{R}^o$, the RNN learns $W \\in \\mathbb{R}^{h \\times d}$, $U \\in \\mathbb{R}^{h \\times h}$, and $V \\in \\mathbb{R}^{o \\times h}$ via backpropagation through time, such that:\n","$$\n","\\begin{align*}\n","z_t &= f(Wx_t + b_W + U z_{t-1} + b_U), \\\\\n","y_t &= g(V z_t + b_V) \\equiv g(y_t'),\n","\\end{align*}\n","$$\n","where $f(\\cdot)$ and $g(\\cdot)$ indicate element-wise nonlinearity; $g(\\cdot)$ is a softmax function in classification tasks. Recall that $y_t'$ is the vector of unnormalized logits.\n"],"metadata":{"id":"Xvo34gKyUZdB"},"id":"Xvo34gKyUZdB"},{"cell_type":"markdown","source":["__A brief historical aside ...__\n","\n","In their 1986 opus (as Kirov and Cotterell call it): [Learning Internal Representations by Error Propagation](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf), David Rumelhart, Geoffrey Hinton, and Ronald Williams write:\n","> \"_We have thus far restricted ourselves to feedforward nets. This may seem like a substantial restriction, but as Minsky and Papert point out, there is, for every recurrent network, a feedforward network with identical behavior (over a finite period of time)._\"\n","\n","They go on to note the following about recurrent networks:\n","> \"[...] _In short, we believe that we have answered\n","Minsky and Papert's challenge and have found a learning result sufficiently powerful to demonstrate that their pessimism about learning in multilayer machines was misplaced._\"\n","\n","For historical notes on RNNs, read Section 3, \"1986 vs. Today\" of [Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate](https://aclanthology.org/Q18-1045.pdf) by Kirov and Cotterell.\n"],"metadata":{"id":"t83UWCwbkiQ9"},"id":"t83UWCwbkiQ9"},{"cell_type":"markdown","source":["<a name=\"sec51\"></a>\n","\n","#### [5.1] Single-layer vanilla RNN <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File to be modified: `ner/nn/models/rnn.py`.</font>\n","\n","In this section, we will implement a simple, single-layered RNN. Just as we did with FFNNs, for this part of the implementation, we will \"forget\" about the existence of `num_layers` (for convenience, we set `num_layers = 1` in the RNN class constructor).\n","\n","__Initializing the RNN__\n","\n","Let's implement the `__init__` part of our `RNN` class to reflect the above. What do we need?:\n","* the transformation matrices $\\mathrm{W}$, $\\mathrm{U}$, $\\mathrm{V}$, each implemented as [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)—this needs to be filled in by you, under <font color=\"orange\">`TODO-5-1`</font>,\n","* a nonlinearity $f(\\cdot)$: the `nonlinearity_dict` class variable offers three options for nonlinearity, [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU), [`nn.Tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html?highlight=tanh#torch.nn.Tanh), and [`nn.PReLU`](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html?highlight=prelu#torch.nn.PReLU); `self.nonlinear` chooses from the `nonlinearity_dict` based on the `nonlinearity` input argument to the class constructor—no need to make any changes here!,\n","* weight initialization of the transformation matrices: already implemented using `self.apply(self.init_weights)`.\n","\n","Feel free to test that your initialization runs as expected in the cell below—we've gone ahead and created a test RNN for you, you can check if the shapes of the model components are as expected."],"metadata":{"id":"Kriqya29_odI"},"id":"Kriqya29_odI"},{"cell_type":"code","source":["from ner.nn.models.rnn import RNN\n","\n","rnn = RNN(embedding_dim=10, hidden_dim=5, output_dim=2, bias=True, nonlinearity=\"tanh\")\n","# Test shapes of rnn.* components."],"metadata":{"id":"eMpKB35j_asa"},"id":"eMpKB35j_asa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__The forward pass (and the initial hidden state)__\n","\n","Now that we've successfully initialized the RNN, let's try to run a `forward()` pass through the network. What does this entail?—we iterate on the _length_ [= time] dimension of the (batched) input embedding of shape `(batch_size, batch_max_length, embedding_dim)`, and at each timestep $t$, we:\n","* use $\\mathrm{W}$ to transform the input $x_t$ to $Wx_t + b_W$,\n","* use $\\mathrm{U}$ to transform the previous hidden state $z_{t-1}$ to $Uz_{t-1} + b_U$,\n","* compute the current hidden state $z_t = f(Wx_t + b_W + Uz_{t-1} + b_U)$, where $f(\\cdot)$ is defined via `self.nonlinear`, and\n","* use $\\mathrm{V}$ to transform the hidden state $z_t$ to output state $y'_t = Vz_t + b_V$ of shape `(batch_size, output_dim)`.\n","\n","Again, going back to our \"Q3.1 thinking\" and noting that we are using `nn.CrossEntropyLoss`, should we use a softmax over $y'_t$?\n","\n","There are two main \"hiccups\" we haven't dealt with (yet!)—how do we \"aggregate\" the outputs obtained at each timestep?, and how do we obtain the hidden state $z_0$, for the first timestep?\n","\n","For output aggregation, special care must be taken to return the output in the expected return type—the return type of the `forward()` method is `torch.Tensor` and not `List[torch.Tensor]`; the final output shape is expected to be `(batch_size, batch_max_length, output_dim)`. There are muliple ways of achieving this, including [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack.html), [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=cat), etc.\n","\n","For the initial hidden state, we provide you with a helper `_initial_hidden_states()` function within the `RNN` class that returns (a list of) initial hidden states, provided a batch size and initialization scheme. Let's see what this function returns! (For convenience, let's reuse the RNN class we created above.)"],"metadata":{"id":"uEUqgXGVAq2m"},"id":"uEUqgXGVAq2m"},{"cell_type":"code","source":["# batch_size = 1, init_zeros = False\n","initial_hiddens = rnn._initial_hidden_states(batch_size=1, init_zeros=False)\n","print(f\"batch_size = 1, init_zeros = False: {initial_hiddens}\")\n","print(initial_hiddens[0].shape, '\\n')\n","\n","# batch_size = 1, init_zeros = True\n","initial_hiddens = rnn._initial_hidden_states(batch_size=1, init_zeros=True)\n","print(f\"batch_size = 1, init_zeros = False: {initial_hiddens}\")\n","print(initial_hiddens[0].shape)"],"metadata":{"id":"rCQ3M0mT6cdo"},"id":"rCQ3M0mT6cdo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One final note: the initial hidden states output by `_initial_hidden_states()` must be on the same device as the RNN model. With this in mind, fill out the <font color=\"orange\">`TODO-5-2`</font> in the `forward()` method to run a forward pass.\n","\n","Upon completion, you can run the cell below to train your RNN! Change the `batch_size` and `num_epochs` below as you see fit; all other hyperparameters (e.g., `embedding_dim`, `hidden_dim`, etc.) are present in `scripts/configs/train_model.yml`—you're free to change these as well. Don't worry too much about the performance—the following is simply a test run. (The model artefacts are stored under `<experiment-name>` subfolder of `CS4740/hw2-fa23/artefacts/experiments` folder.)\n","\n","> __Training time.__ With a batch size of $128$, training a single-layered RNN for one epoch on a Colab CPU takes about ~25mins, while on a Colab T4 GPU it takes ~$2$mins. Owing to hardware limitations, we do not recommend increasing the batch size beyond $128$."],"metadata":{"id":"XBK1xQPy8FYq"},"id":"XBK1xQPy8FYq"},{"cell_type":"code","source":["# Set the batch size and number of training epochs.\n","batch_size = None\n","num_epochs = None"],"metadata":{"id":"U4hfMZmquCVH"},"id":"U4hfMZmquCVH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54cf40d7"},"outputs":[],"source":["model_type = \"rnn\"\n","num_layers = 1\n","\n","experiment_name = f\"model={model_type}_layers={num_layers}_batch={batch_size}\"\n","\n","!train_model.py \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"train_model.yml\")} \\\n","    --tokenizer-config-path={os.path.join(CONFIGS_DIR, \"train_tokenizer.yml\")} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --model-type={model_type} \\\n","    --num-layers={num_layers} \\\n","    --batch-size={batch_size} \\\n","    --num-epochs={num_epochs} \\\n","    --basepath-to-store-results={os.path.join(ARTEFACTS_DIR, \"experiments\")} \\\n","    --experiment-name={experiment_name}"],"id":"54cf40d7"},{"cell_type":"markdown","source":["Same as before, from the saved checkpoints, let us load the best model (if your `num_epochs` was greater than one, else the best model is the only model) based on the `entity_f1` validation performance logged above; set the `best_epoch` below to reflect the epoch (zero-indexed) that resulted in the best model."],"metadata":{"id":"HbrgH-7twH6V"},"id":"HbrgH-7twH6V"},{"cell_type":"code","source":["# Change the best epoch value.\n","best_epoch = None"],"metadata":{"id":"e1r5GFqLwHiH"},"id":"e1r5GFqLwHiH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["config_path = os.path.join(ARTEFACTS_DIR, f\"experiments/{experiment_name}/config.json\")\n","with open(config_path, \"r\") as fp:\n","    config = yaml.safe_load(fp)\n","\n","checkpoint_filename = f\"experiments/{experiment_name}/checkpoints/checkpoint_{best_epoch}.ckpt\"\n","model = NERPredictor(\n","    vocab_size=tokenizer.vocab_size,\n","    padding_idx=tokenizer.token2id[tokenizer.pad_token],\n","    output_dim=len(NER_ENCODING_MAP) - 1,\n","    **config[\"model\"],\n",")\n","checkpoint = torch.load(os.path.join(ARTEFACTS_DIR, checkpoint_filename), map_location=torch.device(\"cpu\"))\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.print_params()"],"metadata":{"id":"jsN8knyRwR_g"},"id":"jsN8knyRwR_g","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how well a learned RNN performs on unseen data; running the cell below shows the model's predictions for a chosen sample (change the `sample_idx` value to retrieve a different sample)."],"metadata":{"id":"z6_sUWXq_pc4"},"id":"z6_sUWXq_pc4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2a17163c"},"outputs":[],"source":["partition, sample_idx = \"val\", 3\n","labels = hf_dataset[partition][sample_idx][\"NER\"] if \"NER\" in hf_dataset[partition][sample_idx] else None\n","_ = inspect_preds(tokenizer=tokenizer, model=model, text=hf_dataset[partition][sample_idx][\"text\"], labels=labels)"],"id":"2a17163c"},{"cell_type":"markdown","source":["<a name=\"sec52\"></a>\n","\n","#### [5.2] Multi-layered RNN <small>[↩︎](#outline)</small>\n","\n","> <font color=\"orange\">File (to be modified): `ner/nn/models/rnn.py` (same file used in [section 5.1](#sec51)).</font>\n","\n","Congrats on running your first RNN! Now, let's go back and adapt our single-layered vanilla RNN into a multi-layered RNN. <font color=\"red\">This is the implementation that you'll be submitting to us (not the single-layered implementation).</font>\n","\n","__Accommodating multiple layers at initialization__\n","\n","In the previous section, we initialized such that we project to and from a single hidden layer; we now wish to support an arbitrary number of hidden layers corresponding to `num_layers`. Note: to keep things simple, we will not be sharing weights across any layers. Let's modify the `__init__` of our RNN class to accommodate this. What do we need to change?:\n","* we still need $W_1$ to transform $x_t$ for $z_{1,t}$ ($z_{1,t}$ indicates the hidden intermediate at first layer and timestep $t$), so let's retain $\\mathrm{W}$ as $\\mathrm{W}_1$,\n","* we need an appropriate number of $\\mathrm{W}_k$s ($k > 1$) that transform the current hidden intermediate $z_{k-1,t} \\in \\mathbb{R}^h$ at layer $k-1$ to an intermediate to be used in layer $k$ at the same timestep $t$—each $\\mathrm{W}_k$ can be implemented as [`nn.Linear`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Linear.html) and we'll maintain the list using an [`nn.ModuleList`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.ModuleList.html) (same as what we did in multi-layered FFNN),\n","  * think _vertical_ (in Fig. 2)!: $\\mathrm{W}_k$ moves vectors from layer $k-1$ to layer $k$, and\n","  * when $k = 1$, there is no $\\mathrm{W}_{k\\,>\\,1}$.\n","* we need an appropriate number of $\\mathrm{U}_k$s that transform the previous hidden intermediate $z_{k,t-1}\\in \\mathbb{R}^h$ at hidden layer $k$ to an intermediate to be used at timestep $t$ in the same layer $k$—each $\\mathrm{U}_k$ can be implemented as [`nn.Linear`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Linear.html) and we'll maintain the list using an [`nn.ModuleList`](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.ModuleList.html),\n","  * think _horizontal_ (in Fig. 2)!: $\\mathrm{U}_k$ is shared across the timesteps of layer $k$, and\n","  * when $k = 1$, the multi-layered RNN should look exactly like vanilla RNN from [section 5.1](#sec51) with $\\mathrm{U}_1$ being vanilla RNN's $\\mathrm{U}$.\n","* we also need $V: z_{n,t}$ → $y_t'$ ($z_{n,t}$ indicates the hidden intermediate from timestep $t$ at the last layer)—let's retain $\\mathrm{V}$.\n","\n","With these changes, your RNN should now accommodate `num_layers`-many hidden layers. Upon completion, run the cell below to see if the model parameters are as expected."],"metadata":{"id":"J_IgF9Ir_dXj"},"id":"J_IgF9Ir_dXj"},{"cell_type":"code","source":["rnn = RNN(embedding_dim=10, hidden_dim=5, output_dim=2, bias=True, nonlinearity=\"tanh\", num_layers=5)\n","rnn.print_params()"],"metadata":{"id":"TTg26f8DJb_t"},"id":"TTg26f8DJb_t","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["__The forward pass, take-2!__\n","\n","Now that we have successfully initialized our RNN to support multiple layers, we need to update the RNN's `forward()` method to forward propagate through _all_ of the hidden layers (not just the first one!). How?—same as before, we iterate on the _length_ [= time] dimension of the (batched) input embedding of shape `(batch_size, batch_max_length, embedding_dim)`, and at each timestep $t$, we:\n","\n","* (same as with single-layered RNN,) use `_initial_hidden_states()` helper to initialize appropriate number of initial hidden states—no change here!,\n","* use $\\mathrm{W}_1$ to transform $x_t$ to $W_1 x_t + b_{W_1}$, $\\mathrm{U}_1$ to transform $z_{1,t-1}$ to $U_1 z_{t-1} + b_{Z_1}$, and compute $z_{1,t}$ accordingly—this is the same as what we did in [section 5.1](#sec51),\n","* for each $k > 1$, use $\\mathrm{W}_k$ to tranform $z_{k-1,t}$ and $\\mathrm{U}_k$ to transform $z_{k,t-1}$, and compute $z_{k,t} = f(W_k z_{k-1,t} + b_{W_k} + U_k z_{k,t-1} + b_{U_k})$—make changes to include this functionality, and\n","* use $\\mathrm{V}$ to tranform the last ($n$-th) hidden intermediate $z_{n,t}$ to $y_t'$—modify the existing code to reflect this.\n","\n","Again, should we softmax $y_t'$ such that $y_t = \\text{softmax}(y'_t)$? As noted in vanilla RNN, take special care to note that the final output is a `torch.Tensor` of shape `(batch_size, batch_max_length, output_dim)` and not a `List[torch.Tensor]`.\n","\n","After making the needed changes to your `forward()` call, you can run the cell below to train your RNN! Change the `num_layers`, `batch_size`, and `num_epochs` below as you see fit; all other hyperparameters (e.g., `embedding_dim`, `hidden_dim`, etc.) are present in `scripts/configs/train_model.yml`—you're free to change these as well. (The model artefacts are stored under `<experiment-name>` subfolder of `CS4740/hw2-fa23/artefacts/experiments` folder.)\n","\n","> __Training time.__ With a batch size of $128$, training a two-layered RNN for one epoch on a Colab CPU takes about ~30mins, while on a Colab T4 GPU it takes ~$3$mins. Owing to hardware limitations, we do not recommend increasing the batch size beyond $128$.\n","\n","<font color=\"red\">Do not use `num_layers` greater than $2$; using more than two layers causes unwanted out-of-memory errors on our autograder servers.</font> (You are free to experiment with more than two layers, but the models in the final submission __cannot__ have more than two layers.)"],"metadata":{"id":"ao6SpskBJpjN"},"id":"ao6SpskBJpjN"},{"cell_type":"code","source":["# Set the number of layers, batch size, and number of training epochs.\n","num_layers = None\n","batch_size = None\n","num_epochs = None"],"metadata":{"id":"WKHKsPfFbEEL"},"id":"WKHKsPfFbEEL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_type = \"rnn\"\n","\n","experiment_name = f\"model={model_type}_layers={num_layers}_batch={batch_size}\"\n","\n","!train_model.py \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"train_model.yml\")} \\\n","    --tokenizer-config-path={os.path.join(CONFIGS_DIR, \"train_tokenizer.yml\")} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --model-type={model_type} \\\n","    --num-layers={num_layers} \\\n","    --batch-size={batch_size} \\\n","    --num-epochs={num_epochs} \\\n","    --basepath-to-store-results={os.path.join(ARTEFACTS_DIR, \"experiments\")} \\\n","    --experiment-name={experiment_name}"],"metadata":{"id":"PWKhndqxJpLl"},"id":"PWKhndqxJpLl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just as before, from the saved checkpoints, let's load the best model (if your num_epochs was more than one, else the best model is the only model) based on the `entity_f1` validation performance logged above; set the `best_epoch` below to reflect the epoch (zero-indexed) that resulted in the best model."],"metadata":{"id":"12PeZ6S0bfth"},"id":"12PeZ6S0bfth"},{"cell_type":"code","source":["# Change the best epoch value.\n","best_epoch = None"],"metadata":{"id":"bV3yMu4gbpSL"},"id":"bV3yMu4gbpSL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["config_path = os.path.join(ARTEFACTS_DIR, f\"experiments/{experiment_name}/config.json\")\n","with open(config_path, \"r\") as fp:\n","    config = yaml.safe_load(fp)\n","\n","checkpoint_filename = f\"experiments/{experiment_name}/checkpoints/checkpoint_{best_epoch}.ckpt\"\n","model = NERPredictor(\n","    vocab_size=tokenizer.vocab_size,\n","    padding_idx=tokenizer.token2id[tokenizer.pad_token],\n","    output_dim=len(NER_ENCODING_MAP) - 1,\n","    **config[\"model\"],\n",")\n","checkpoint = torch.load(os.path.join(ARTEFACTS_DIR, checkpoint_filename), map_location=torch.device(\"cpu\"))\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.print_params()"],"metadata":{"id":"057OuG_cbpuS"},"id":"057OuG_cbpuS","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how well our multi-layered RNN performs on unseen data; running the cell below shows the model's predictions for a chosen sample (change the `sample_idx` value to retrieve a different sample)."],"metadata":{"id":"pe-E1lHVbthX"},"id":"pe-E1lHVbthX"},{"cell_type":"code","source":["partition, sample_idx = \"val\", 3\n","labels = hf_dataset[partition][sample_idx][\"NER\"] if \"NER\" in hf_dataset[partition][sample_idx] else None\n","_ = inspect_preds(tokenizer=tokenizer, model=model, text=hf_dataset[partition][sample_idx][\"text\"], labels=labels)"],"metadata":{"id":"wQppwC_jb07W"},"id":"wQppwC_jb07W","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just as we did with multi-layered FFNN, we can visualize the activations from any specific module in our multi-layered RNN. Set the `module` variable below to visualize the activations of a specific module within your model—use the syntax: `<model-varname>.model.<module-name>` (e.g., `model.model.V` to visualize a layer named `V` in your RNN named `model`)."],"metadata":{"id":"RzOs_QCnb_tj"},"id":"RzOs_QCnb_tj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1770d4a4"},"outputs":[],"source":["# Set the module you wish to visualize (format: model.model.<module-name>).\n","module = None\n","\n","visualize_activations(\n","    tokenizer=tokenizer,\n","    model=model,\n","    module=module,\n","    text=hf_dataset[partition][sample_idx][\"text\"],\n","    labels=labels,\n","    nonlinearity=F.relu,\n",")"],"id":"1770d4a4"},{"cell_type":"markdown","source":["<a name=\"leaderboard_rnn\"></a>\n","\n","#### [$\\ast$] Leaderboard submission <small>[↩︎](#outline)</small>\n","\n","> __Note.__ Submitting to the leaderboard is optional, see [<sup>[2]</sup>](#optional) for baselines and related information.\n","\n","Let's make a leaderboard submission using our trained RNN. Run the following cells to make a leaderboard submission. As noted before, the `make_submission` command when run with `--leaderboard-submission` flag creates a `leaderboard_submission.zip` file in `CS4740/hw2-fa23/artefacts` folder, which is to be submitted on the submission site. <font color=\"red\">Caution: the script will overwrite any file named `leaderboard_submission.zip` existing in `CS4740/hw2-fa23/artefacts` folder.</font>\n","\n","> __Tip.__ When we made a [leaderboard submission for FFNN](#leaderboard_ffnn), we ran the `train_model.py` script by setting the `--ffnn-config-path` and `--pretrained-ffnn-checkpoint-or-model-filepath` flags; these flags can be set alongside `--rnn-config-path` and `--pretrained-rnn-checkpoint-or-model-filepath` to make a concurrent FFNN and RNN leaderboard submission! <br/>\n","> (Run the command in [final submission](#final) section __with__ `--leaderboard-submission` flag.)\n","\n","Set the `rnn_experiment_name` and `rnn_best_epoch` accordingly. The `leaderboard_submission.zip` is all that you will need to submit to the leaderboard (no need to submit anything else!)."],"metadata":{"id":"fk2ya6eii4DL"},"id":"fk2ya6eii4DL"},{"cell_type":"code","source":["# Set the following accordingly.\n","rnn_experiment_name = None\n","rnn_best_epoch = None"],"metadata":{"id":"O6utvi75j64T"},"id":"O6utvi75j64T","execution_count":null,"outputs":[]},{"cell_type":"code","source":["submission_filepath = f\"{ARTEFACTS_DIR}\"\n","\n","rnn_config_filename =  f\"experiments/{rnn_experiment_name}/config.json\"\n","rnn_checkpoint_filename = f\"experiments/{rnn_experiment_name}/checkpoints/checkpoint_{ffnn_best_epoch}.ckpt\"\n","\n","!make_submission.py \\\n","    --rnn-config-path={os.path.join(ARTEFACTS_DIR, rnn_config_filename)} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --basepath-to-store-submission={os.path.join(ARTEFACTS_DIR, submission_filepath)} \\\n","    --pretrained-rnn-checkpoint-or-model-filepath={os.path.join(ARTEFACTS_DIR, rnn_checkpoint_filename)} \\\n","    --leaderboard-submission\n","\n","if os.path.isfile(f\"{os.path.join(ARTEFACTS_DIR, 'leaderboard_submission.zip')}\"):\n","    display(success())\n","else:\n","    print(colored(\"Oops, something went wrong!\", \"red\"))"],"metadata":{"id":"7Xvkz_1NkUcd"},"id":"7Xvkz_1NkUcd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name=\"sec53\"></a>\n","\n","#### [5.3] Analyzing RNN <small>[↩︎](#outline)</small>\n","\n","Again, be concise when answer the following questions. The goal of these questions is to get you thinking about designing and training RNNs, and are _not_ meant to be an unordered collection of all your thoughts about RNNs. Make compelling (preferably, data-backed) arguments that aren't misleading or confusing.\n","\n","(Save for the optional question, all other questions in this section must be answered in under $1.5$ pages.)"],"metadata":{"id":"aOKK0j9p6hrk"},"id":"aOKK0j9p6hrk"},{"cell_type":"markdown","source":["---\n","<font color=\"orange\">__Q5.3.1.__ In comparison to your best FFNN model, how did your best RNN model _perform_? Again, performance is more than just \"performance on some metric\"; it includes efficiency (e.g., training time, convergence rate), memory (e.g., weights storage), generalizability (e.g., on unknown words), etc. Choose any two dimensions and present your views.\n","</font>"],"metadata":{"id":"6Xo0xiSF9Kum"},"id":"6Xo0xiSF9Kum"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"uzY0sKdL9sS9"},"id":"uzY0sKdL9sS9"},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__Q5.3.2.__ From the (averaged) entity-level F1 score, we realize that RNN is far better performing than an FFNN (for the underlying task). Ignoring the training time, what happens if we used an RNN to process a really long sequence, say $O(2^{12})$ tokens (most large language models operate at this order)? <br/>\n","_Hint. Think recurrence! How much information from the first few tokens is retained in the last few timesteps?_\n","</font>"],"metadata":{"id":"EYTVvrRh-DZK"},"id":"EYTVvrRh-DZK"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"_Hvx4RCrB-f-"},"id":"_Hvx4RCrB-f-"},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__Q5.3.3.__ Consider the word \"Bank\" in two sequences: 1) \"_I went to <u>Bank</u> of America to talk to the manager_\", and 2) \"_I went to <u>Bank</u> to draw cash_\". Would our current RNN model be able to accurately classify \"Bank\" in sequence-1 as \"B-LOC\" and the \"Bank\" in sequence-2 as \"O\"? If your answer is _yes_, then justify your answer; if your answer is _no_, then provide a suitable fix. <br/>\n","_Hint. Think about how our RNN model processes an input sequence._\n","</font>"],"metadata":{"id":"WC8yp2Mr9vec"},"id":"WC8yp2Mr9vec"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"4GU2mfiWCDDi"},"id":"4GU2mfiWCDDi"},{"cell_type":"markdown","source":["---\n","\n","<font color=\"orange\">__[optional, ungraded] Q5.3.4.__ Visualize the activations of the modules in your trained RNN. See how activations for named-entities vary (when compared to non named-entities) as you pass through the layers of your multi-layered RNN. Are there any interesting patterns? <br/>\n","Again, we're not looking for one example where everthing (by some stroke of luck) looks \"nice\"; look for any interesting and general patterns.\n","</font>"],"metadata":{"id":"DP3qkQlAC7WZ"},"id":"DP3qkQlAC7WZ"},{"cell_type":"markdown","source":["__Answer.__\n","\n","---"],"metadata":{"id":"2IxT2lerDPJk"},"id":"2IxT2lerDPJk"},{"cell_type":"markdown","metadata":{"id":"3d1c981e"},"source":["---\n","<a name=\"final\"></a>\n","\n","### [$\\ast$] Final submission <small>[↩︎](#outline)</small>\n","\n","Hurray! Now that we've succesfully trained our FFNN and RNN, let's bundle everything up and make a submission on the submission site(s). Running the cell below will generate a `hw2_submission.zip` file in the `CS4740/hw2-fa23/artefacts` folder.  <font color=\"red\">Caution: the script will overwrite any file named `hw2_submission.zip` existing in `CS4740/hw2-fa23/artefacts` folder.</font>\n","\n","Before running the cells below, set the `ffnn_experiment_name`, `ffnn_best_epoch`, `rnn_experiment_name`, and `rnn_best_epoch` accordingly. You will need to submit the `hw2_submission.zip` __and a .pdf version of this notebook file__ on the submission site(s). Note: this notebook will only be used to grade your answers to the written questions; you will _not_ be graded on any code in this notebook file."],"id":"3d1c981e"},{"cell_type":"code","source":["# Set the following regarding your FFNN model.\n","ffnn_experiment_name = None\n","ffnn_best_epoch = None"],"metadata":{"id":"gI4XvgWnn06v"},"id":"gI4XvgWnn06v","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set the following regarding your RNN model.\n","rnn_experiment_name = None\n","rnn_best_epoch = None"],"metadata":{"id":"IqqcxpSMn3tQ"},"id":"IqqcxpSMn3tQ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44cfe4ce"},"outputs":[],"source":["submission_filepath = f\"{ARTEFACTS_DIR}\"\n","\n","ffnn_config_filename =  f\"experiments/{ffnn_experiment_name}/config.json\"\n","ffnn_checkpoint_filename = f\"experiments/{ffnn_experiment_name}/checkpoints/checkpoint_{ffnn_best_epoch}.ckpt\"\n","\n","rnn_config_filename =  f\"experiments/{rnn_experiment_name}/config.json\"\n","rnn_checkpoint_filename = f\"experiments/{rnn_experiment_name}/checkpoints/checkpoint_{rnn_best_epoch}.ckpt\"\n","\n","!make_submission.py \\\n","    --ffnn-config-path={os.path.join(ARTEFACTS_DIR, ffnn_config_filename)} \\\n","    --rnn-config-path={os.path.join(ARTEFACTS_DIR, rnn_config_filename)} \\\n","    --basepath-to-hf-dataset={os.path.join(ARTEFACTS_DIR, \"dataset\")} \\\n","    --tokenizer-filepath={os.path.join(ARTEFACTS_DIR, \"tokenizer/tokenizer.json\")} \\\n","    --basepath-to-store-submission={os.path.join(ARTEFACTS_DIR, submission_filepath)} \\\n","    --pretrained-ffnn-checkpoint-or-model-filepath={os.path.join(ARTEFACTS_DIR, ffnn_checkpoint_filename)} \\\n","    --pretrained-rnn-checkpoint-or-model-filepath={os.path.join(ARTEFACTS_DIR, rnn_checkpoint_filename)} \\\n","    --net-ids={net_ids}\n","\n","if os.path.isfile(f\"{os.path.join(ARTEFACTS_DIR, 'hw2_submission.zip')}\"):\n","    display(success())\n","else:\n","    print(colored(\"Oops, something went wrong!\", \"red\"))"],"id":"44cfe4ce"},{"cell_type":"code","source":[],"metadata":{"id":"aRP0u6D5EEh0"},"id":"aRP0u6D5EEh0","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}